{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "xs = np.array([\r\n",
    "    0., 0.,\r\n",
    "    0., 1.,\r\n",
    "    1., 0.,\r\n",
    "    1., 1.\r\n",
    "]).reshape(4, 2)\r\n",
    "\r\n",
    "ys = np.array([0., 1., 1., 0.]) #.reshape(4,)\r\n",
    "\r\n",
    "# ys = (ys - np.mean(ys)) / np.std(ys)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "xs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "ys"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "model = MLPRegressor(activation='relu', max_iter=10000, hidden_layer_sizes=(4,4), random_state=1, verbose=True, early_stopping=False, n_iter_no_change=100, batch_size=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "model = model.fit(xs, ys)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.12843934\n",
      "Iteration 2, loss = 0.12768771\n",
      "Iteration 3, loss = 0.12773889\n",
      "Iteration 4, loss = 0.12754232\n",
      "Iteration 5, loss = 0.12750285\n",
      "Iteration 6, loss = 0.12746590\n",
      "Iteration 7, loss = 0.12739870\n",
      "Iteration 8, loss = 0.12735286\n",
      "Iteration 9, loss = 0.12725038\n",
      "Iteration 10, loss = 0.12735942\n",
      "Iteration 11, loss = 0.12723274\n",
      "Iteration 12, loss = 0.12704889\n",
      "Iteration 13, loss = 0.12700131\n",
      "Iteration 14, loss = 0.12693676\n",
      "Iteration 15, loss = 0.12686686\n",
      "Iteration 16, loss = 0.12682526\n",
      "Iteration 17, loss = 0.12685195\n",
      "Iteration 18, loss = 0.12674329\n",
      "Iteration 19, loss = 0.12661834\n",
      "Iteration 20, loss = 0.12656897\n",
      "Iteration 21, loss = 0.12663178\n",
      "Iteration 22, loss = 0.12641506\n",
      "Iteration 23, loss = 0.12634565\n",
      "Iteration 24, loss = 0.12632158\n",
      "Iteration 25, loss = 0.12626459\n",
      "Iteration 26, loss = 0.12623362\n",
      "Iteration 27, loss = 0.12614061\n",
      "Iteration 28, loss = 0.12609184\n",
      "Iteration 29, loss = 0.12614058\n",
      "Iteration 30, loss = 0.12594997\n",
      "Iteration 31, loss = 0.12588772\n",
      "Iteration 32, loss = 0.12587624\n",
      "Iteration 33, loss = 0.12578455\n",
      "Iteration 34, loss = 0.12569372\n",
      "Iteration 35, loss = 0.12571021\n",
      "Iteration 36, loss = 0.12563633\n",
      "Iteration 37, loss = 0.12551615\n",
      "Iteration 38, loss = 0.12566197\n",
      "Iteration 39, loss = 0.12539852\n",
      "Iteration 40, loss = 0.12539985\n",
      "Iteration 41, loss = 0.12537432\n",
      "Iteration 42, loss = 0.12529026\n",
      "Iteration 43, loss = 0.12536280\n",
      "Iteration 44, loss = 0.12518109\n",
      "Iteration 45, loss = 0.12505108\n",
      "Iteration 46, loss = 0.12505107\n",
      "Iteration 47, loss = 0.12499349\n",
      "Iteration 48, loss = 0.12494548\n",
      "Iteration 49, loss = 0.12488120\n",
      "Iteration 50, loss = 0.12489945\n",
      "Iteration 51, loss = 0.12480650\n",
      "Iteration 52, loss = 0.12481711\n",
      "Iteration 53, loss = 0.12460452\n",
      "Iteration 54, loss = 0.12454451\n",
      "Iteration 55, loss = 0.12449747\n",
      "Iteration 56, loss = 0.12449176\n",
      "Iteration 57, loss = 0.12438419\n",
      "Iteration 58, loss = 0.12429860\n",
      "Iteration 59, loss = 0.12430095\n",
      "Iteration 60, loss = 0.12429027\n",
      "Iteration 61, loss = 0.12417318\n",
      "Iteration 62, loss = 0.12417330\n",
      "Iteration 63, loss = 0.12413054\n",
      "Iteration 64, loss = 0.12417038\n",
      "Iteration 65, loss = 0.12422937\n",
      "Iteration 66, loss = 0.12403817\n",
      "Iteration 67, loss = 0.12412604\n",
      "Iteration 68, loss = 0.12408725\n",
      "Iteration 69, loss = 0.12397637\n",
      "Iteration 70, loss = 0.12396394\n",
      "Iteration 71, loss = 0.12393668\n",
      "Iteration 72, loss = 0.12389586\n",
      "Iteration 73, loss = 0.12388576\n",
      "Iteration 74, loss = 0.12383713\n",
      "Iteration 75, loss = 0.12399219\n",
      "Iteration 76, loss = 0.12380411\n",
      "Iteration 77, loss = 0.12378785\n",
      "Iteration 78, loss = 0.12376546\n",
      "Iteration 79, loss = 0.12374319\n",
      "Iteration 80, loss = 0.12370973\n",
      "Iteration 81, loss = 0.12384419\n",
      "Iteration 82, loss = 0.12366898\n",
      "Iteration 83, loss = 0.12375454\n",
      "Iteration 84, loss = 0.12359704\n",
      "Iteration 85, loss = 0.12364926\n",
      "Iteration 86, loss = 0.12361526\n",
      "Iteration 87, loss = 0.12354047\n",
      "Iteration 88, loss = 0.12362472\n",
      "Iteration 89, loss = 0.12349753\n",
      "Iteration 90, loss = 0.12349695\n",
      "Iteration 91, loss = 0.12352760\n",
      "Iteration 92, loss = 0.12356667\n",
      "Iteration 93, loss = 0.12339312\n",
      "Iteration 94, loss = 0.12348890\n",
      "Iteration 95, loss = 0.12334260\n",
      "Iteration 96, loss = 0.12331541\n",
      "Iteration 97, loss = 0.12328582\n",
      "Iteration 98, loss = 0.12327883\n",
      "Iteration 99, loss = 0.12326637\n",
      "Iteration 100, loss = 0.12320647\n",
      "Iteration 101, loss = 0.12330233\n",
      "Iteration 102, loss = 0.12325401\n",
      "Iteration 103, loss = 0.12320186\n",
      "Iteration 104, loss = 0.12314257\n",
      "Iteration 105, loss = 0.12314724\n",
      "Iteration 106, loss = 0.12302623\n",
      "Iteration 107, loss = 0.12319199\n",
      "Iteration 108, loss = 0.12300375\n",
      "Iteration 109, loss = 0.12304328\n",
      "Iteration 110, loss = 0.12298945\n",
      "Iteration 111, loss = 0.12293803\n",
      "Iteration 112, loss = 0.12303254\n",
      "Iteration 113, loss = 0.12293792\n",
      "Iteration 114, loss = 0.12280001\n",
      "Iteration 115, loss = 0.12278631\n",
      "Iteration 116, loss = 0.12275510\n",
      "Iteration 117, loss = 0.12285153\n",
      "Iteration 118, loss = 0.12270193\n",
      "Iteration 119, loss = 0.12276199\n",
      "Iteration 120, loss = 0.12270522\n",
      "Iteration 121, loss = 0.12264085\n",
      "Iteration 122, loss = 0.12273446\n",
      "Iteration 123, loss = 0.12254046\n",
      "Iteration 124, loss = 0.12249819\n",
      "Iteration 125, loss = 0.12249723\n",
      "Iteration 126, loss = 0.12244174\n",
      "Iteration 127, loss = 0.12243231\n",
      "Iteration 128, loss = 0.12239436\n",
      "Iteration 129, loss = 0.12235829\n",
      "Iteration 130, loss = 0.12233107\n",
      "Iteration 131, loss = 0.12232737\n",
      "Iteration 132, loss = 0.12224830\n",
      "Iteration 133, loss = 0.12224312\n",
      "Iteration 134, loss = 0.12218672\n",
      "Iteration 135, loss = 0.12231029\n",
      "Iteration 136, loss = 0.12220510\n",
      "Iteration 137, loss = 0.12220740\n",
      "Iteration 138, loss = 0.12207737\n",
      "Iteration 139, loss = 0.12211272\n",
      "Iteration 140, loss = 0.12205577\n",
      "Iteration 141, loss = 0.12213451\n",
      "Iteration 142, loss = 0.12192262\n",
      "Iteration 143, loss = 0.12196594\n",
      "Iteration 144, loss = 0.12185326\n",
      "Iteration 145, loss = 0.12181919\n",
      "Iteration 146, loss = 0.12187013\n",
      "Iteration 147, loss = 0.12171905\n",
      "Iteration 148, loss = 0.12175367\n",
      "Iteration 149, loss = 0.12162166\n",
      "Iteration 150, loss = 0.12164718\n",
      "Iteration 151, loss = 0.12160465\n",
      "Iteration 152, loss = 0.12161248\n",
      "Iteration 153, loss = 0.12150092\n",
      "Iteration 154, loss = 0.12161717\n",
      "Iteration 155, loss = 0.12143644\n",
      "Iteration 156, loss = 0.12138647\n",
      "Iteration 157, loss = 0.12134943\n",
      "Iteration 158, loss = 0.12129082\n",
      "Iteration 159, loss = 0.12125068\n",
      "Iteration 160, loss = 0.12130920\n",
      "Iteration 161, loss = 0.12117905\n",
      "Iteration 162, loss = 0.12116780\n",
      "Iteration 163, loss = 0.12111540\n",
      "Iteration 164, loss = 0.12105843\n",
      "Iteration 165, loss = 0.12099731\n",
      "Iteration 166, loss = 0.12095752\n",
      "Iteration 167, loss = 0.12104069\n",
      "Iteration 168, loss = 0.12099232\n",
      "Iteration 169, loss = 0.12097272\n",
      "Iteration 170, loss = 0.12077487\n",
      "Iteration 171, loss = 0.12074619\n",
      "Iteration 172, loss = 0.12069146\n",
      "Iteration 173, loss = 0.12074807\n",
      "Iteration 174, loss = 0.12075605\n",
      "Iteration 175, loss = 0.12055847\n",
      "Iteration 176, loss = 0.12051944\n",
      "Iteration 177, loss = 0.12055365\n",
      "Iteration 178, loss = 0.12040980\n",
      "Iteration 179, loss = 0.12035894\n",
      "Iteration 180, loss = 0.12035029\n",
      "Iteration 181, loss = 0.12042141\n",
      "Iteration 182, loss = 0.12021293\n",
      "Iteration 183, loss = 0.12016930\n",
      "Iteration 184, loss = 0.12013466\n",
      "Iteration 185, loss = 0.12006247\n",
      "Iteration 186, loss = 0.12003447\n",
      "Iteration 187, loss = 0.11995674\n",
      "Iteration 188, loss = 0.11990590\n",
      "Iteration 189, loss = 0.11985332\n",
      "Iteration 190, loss = 0.11986394\n",
      "Iteration 191, loss = 0.11978222\n",
      "Iteration 192, loss = 0.11971324\n",
      "Iteration 193, loss = 0.11970737\n",
      "Iteration 194, loss = 0.11964081\n",
      "Iteration 195, loss = 0.11958082\n",
      "Iteration 196, loss = 0.11945953\n",
      "Iteration 197, loss = 0.11939596\n",
      "Iteration 198, loss = 0.11938214\n",
      "Iteration 199, loss = 0.11930600\n",
      "Iteration 200, loss = 0.11940555\n",
      "Iteration 201, loss = 0.11921321\n",
      "Iteration 202, loss = 0.11917468\n",
      "Iteration 203, loss = 0.11915715\n",
      "Iteration 204, loss = 0.11919610\n",
      "Iteration 205, loss = 0.11899105\n",
      "Iteration 206, loss = 0.11897245\n",
      "Iteration 207, loss = 0.11899686\n",
      "Iteration 208, loss = 0.11891311\n",
      "Iteration 209, loss = 0.11875872\n",
      "Iteration 210, loss = 0.11871416\n",
      "Iteration 211, loss = 0.11874223\n",
      "Iteration 212, loss = 0.11864683\n",
      "Iteration 213, loss = 0.11854805\n",
      "Iteration 214, loss = 0.11856840\n",
      "Iteration 215, loss = 0.11843271\n",
      "Iteration 216, loss = 0.11834707\n",
      "Iteration 217, loss = 0.11828242\n",
      "Iteration 218, loss = 0.11822697\n",
      "Iteration 219, loss = 0.11817640\n",
      "Iteration 220, loss = 0.11811050\n",
      "Iteration 221, loss = 0.11805712\n",
      "Iteration 222, loss = 0.11799711\n",
      "Iteration 223, loss = 0.11792279\n",
      "Iteration 224, loss = 0.11795106\n",
      "Iteration 225, loss = 0.11782835\n",
      "Iteration 226, loss = 0.11772100\n",
      "Iteration 227, loss = 0.11775427\n",
      "Iteration 228, loss = 0.11763767\n",
      "Iteration 229, loss = 0.11760317\n",
      "Iteration 230, loss = 0.11749884\n",
      "Iteration 231, loss = 0.11739715\n",
      "Iteration 232, loss = 0.11740085\n",
      "Iteration 233, loss = 0.11742137\n",
      "Iteration 234, loss = 0.11724541\n",
      "Iteration 235, loss = 0.11716207\n",
      "Iteration 236, loss = 0.11707454\n",
      "Iteration 237, loss = 0.11702076\n",
      "Iteration 238, loss = 0.11693095\n",
      "Iteration 239, loss = 0.11686503\n",
      "Iteration 240, loss = 0.11679529\n",
      "Iteration 241, loss = 0.11682628\n",
      "Iteration 242, loss = 0.11671763\n",
      "Iteration 243, loss = 0.11662527\n",
      "Iteration 244, loss = 0.11654396\n",
      "Iteration 245, loss = 0.11648936\n",
      "Iteration 246, loss = 0.11639293\n",
      "Iteration 247, loss = 0.11639056\n",
      "Iteration 248, loss = 0.11622895\n",
      "Iteration 249, loss = 0.11613969\n",
      "Iteration 250, loss = 0.11608006\n",
      "Iteration 251, loss = 0.11604935\n",
      "Iteration 252, loss = 0.11598311\n",
      "Iteration 253, loss = 0.11587570\n",
      "Iteration 254, loss = 0.11579802\n",
      "Iteration 255, loss = 0.11569032\n",
      "Iteration 256, loss = 0.11570607\n",
      "Iteration 257, loss = 0.11558635\n",
      "Iteration 258, loss = 0.11548699\n",
      "Iteration 259, loss = 0.11542992\n",
      "Iteration 260, loss = 0.11537171\n",
      "Iteration 261, loss = 0.11525834\n",
      "Iteration 262, loss = 0.11518350\n",
      "Iteration 263, loss = 0.11506979\n",
      "Iteration 264, loss = 0.11514291\n",
      "Iteration 265, loss = 0.11499596\n",
      "Iteration 266, loss = 0.11498999\n",
      "Iteration 267, loss = 0.11475422\n",
      "Iteration 268, loss = 0.11476166\n",
      "Iteration 269, loss = 0.11461193\n",
      "Iteration 270, loss = 0.11462944\n",
      "Iteration 271, loss = 0.11458080\n",
      "Iteration 272, loss = 0.11433481\n",
      "Iteration 273, loss = 0.11426121\n",
      "Iteration 274, loss = 0.11423424\n",
      "Iteration 275, loss = 0.11409406\n",
      "Iteration 276, loss = 0.11405104\n",
      "Iteration 277, loss = 0.11395450\n",
      "Iteration 278, loss = 0.11394440\n",
      "Iteration 279, loss = 0.11392219\n",
      "Iteration 280, loss = 0.11372113\n",
      "Iteration 281, loss = 0.11361418\n",
      "Iteration 282, loss = 0.11348614\n",
      "Iteration 283, loss = 0.11347484\n",
      "Iteration 284, loss = 0.11337175\n",
      "Iteration 285, loss = 0.11326062\n",
      "Iteration 286, loss = 0.11317892\n",
      "Iteration 287, loss = 0.11305366\n",
      "Iteration 288, loss = 0.11298110\n",
      "Iteration 289, loss = 0.11291233\n",
      "Iteration 290, loss = 0.11283997\n",
      "Iteration 291, loss = 0.11269399\n",
      "Iteration 292, loss = 0.11266259\n",
      "Iteration 293, loss = 0.11266400\n",
      "Iteration 294, loss = 0.11244125\n",
      "Iteration 295, loss = 0.11242590\n",
      "Iteration 296, loss = 0.11224613\n",
      "Iteration 297, loss = 0.11218289\n",
      "Iteration 298, loss = 0.11206456\n",
      "Iteration 299, loss = 0.11201436\n",
      "Iteration 300, loss = 0.11189261\n",
      "Iteration 301, loss = 0.11184433\n",
      "Iteration 302, loss = 0.11170025\n",
      "Iteration 303, loss = 0.11160180\n",
      "Iteration 304, loss = 0.11154396\n",
      "Iteration 305, loss = 0.11153737\n",
      "Iteration 306, loss = 0.11136603\n",
      "Iteration 307, loss = 0.11130116\n",
      "Iteration 308, loss = 0.11115739\n",
      "Iteration 309, loss = 0.11099830\n",
      "Iteration 310, loss = 0.11095557\n",
      "Iteration 311, loss = 0.11080741\n",
      "Iteration 312, loss = 0.11070691\n",
      "Iteration 313, loss = 0.11064883\n",
      "Iteration 314, loss = 0.11054437\n",
      "Iteration 315, loss = 0.11053785\n",
      "Iteration 316, loss = 0.11032627\n",
      "Iteration 317, loss = 0.11039934\n",
      "Iteration 318, loss = 0.11022412\n",
      "Iteration 319, loss = 0.11004990\n",
      "Iteration 320, loss = 0.11002581\n",
      "Iteration 321, loss = 0.10990016\n",
      "Iteration 322, loss = 0.10974191\n",
      "Iteration 323, loss = 0.10960614\n",
      "Iteration 324, loss = 0.10953133\n",
      "Iteration 325, loss = 0.10946590\n",
      "Iteration 326, loss = 0.10930323\n",
      "Iteration 327, loss = 0.10920705\n",
      "Iteration 328, loss = 0.10921762\n",
      "Iteration 329, loss = 0.10901960\n",
      "Iteration 330, loss = 0.10894884\n",
      "Iteration 331, loss = 0.10889367\n",
      "Iteration 332, loss = 0.10868997\n",
      "Iteration 333, loss = 0.10865703\n",
      "Iteration 334, loss = 0.10848307\n",
      "Iteration 335, loss = 0.10840895\n",
      "Iteration 336, loss = 0.10834954\n",
      "Iteration 337, loss = 0.10821341\n",
      "Iteration 338, loss = 0.10813574\n",
      "Iteration 339, loss = 0.10794763\n",
      "Iteration 340, loss = 0.10789616\n",
      "Iteration 341, loss = 0.10779410\n",
      "Iteration 342, loss = 0.10765798\n",
      "Iteration 343, loss = 0.10757719\n",
      "Iteration 344, loss = 0.10747700\n",
      "Iteration 345, loss = 0.10734019\n",
      "Iteration 346, loss = 0.10718484\n",
      "Iteration 347, loss = 0.10705908\n",
      "Iteration 348, loss = 0.10705814\n",
      "Iteration 349, loss = 0.10703792\n",
      "Iteration 350, loss = 0.10688342\n",
      "Iteration 351, loss = 0.10680311\n",
      "Iteration 352, loss = 0.10658989\n",
      "Iteration 353, loss = 0.10654167\n",
      "Iteration 354, loss = 0.10637098\n",
      "Iteration 355, loss = 0.10629669\n",
      "Iteration 356, loss = 0.10611855\n",
      "Iteration 357, loss = 0.10608579\n",
      "Iteration 358, loss = 0.10597069\n",
      "Iteration 359, loss = 0.10579428\n",
      "Iteration 360, loss = 0.10568776\n",
      "Iteration 361, loss = 0.10555317\n",
      "Iteration 362, loss = 0.10550950\n",
      "Iteration 363, loss = 0.10530645\n",
      "Iteration 364, loss = 0.10527997\n",
      "Iteration 365, loss = 0.10514331\n",
      "Iteration 366, loss = 0.10501335\n",
      "Iteration 367, loss = 0.10488314\n",
      "Iteration 368, loss = 0.10483734\n",
      "Iteration 369, loss = 0.10477404\n",
      "Iteration 370, loss = 0.10456903\n",
      "Iteration 371, loss = 0.10460647\n",
      "Iteration 372, loss = 0.10444886\n",
      "Iteration 373, loss = 0.10422092\n",
      "Iteration 374, loss = 0.10421848\n",
      "Iteration 375, loss = 0.10398110\n",
      "Iteration 376, loss = 0.10403521\n",
      "Iteration 377, loss = 0.10378302\n",
      "Iteration 378, loss = 0.10364791\n",
      "Iteration 379, loss = 0.10352690\n",
      "Iteration 380, loss = 0.10348179\n",
      "Iteration 381, loss = 0.10331835\n",
      "Iteration 382, loss = 0.10320793\n",
      "Iteration 383, loss = 0.10310378\n",
      "Iteration 384, loss = 0.10299183\n",
      "Iteration 385, loss = 0.10283522\n",
      "Iteration 386, loss = 0.10272193\n",
      "Iteration 387, loss = 0.10265427\n",
      "Iteration 388, loss = 0.10253997\n",
      "Iteration 389, loss = 0.10238822\n",
      "Iteration 390, loss = 0.10229381\n",
      "Iteration 391, loss = 0.10219362\n",
      "Iteration 392, loss = 0.10204300\n",
      "Iteration 393, loss = 0.10195174\n",
      "Iteration 394, loss = 0.10179147\n",
      "Iteration 395, loss = 0.10166467\n",
      "Iteration 396, loss = 0.10162339\n",
      "Iteration 397, loss = 0.10145177\n",
      "Iteration 398, loss = 0.10133459\n",
      "Iteration 399, loss = 0.10131854\n",
      "Iteration 400, loss = 0.10115169\n",
      "Iteration 401, loss = 0.10107187\n",
      "Iteration 402, loss = 0.10089632\n",
      "Iteration 403, loss = 0.10078474\n",
      "Iteration 404, loss = 0.10071184\n",
      "Iteration 405, loss = 0.10062678\n",
      "Iteration 406, loss = 0.10047325\n",
      "Iteration 407, loss = 0.10028152\n",
      "Iteration 408, loss = 0.10020280\n",
      "Iteration 409, loss = 0.10014716\n",
      "Iteration 410, loss = 0.10004218\n",
      "Iteration 411, loss = 0.09990798\n",
      "Iteration 412, loss = 0.09969734\n",
      "Iteration 413, loss = 0.09966160\n",
      "Iteration 414, loss = 0.09946180\n",
      "Iteration 415, loss = 0.09943674\n",
      "Iteration 416, loss = 0.09935102\n",
      "Iteration 417, loss = 0.09925968\n",
      "Iteration 418, loss = 0.09901857\n",
      "Iteration 419, loss = 0.09898459\n",
      "Iteration 420, loss = 0.09884809\n",
      "Iteration 421, loss = 0.09876411\n",
      "Iteration 422, loss = 0.09862191\n",
      "Iteration 423, loss = 0.09853864\n",
      "Iteration 424, loss = 0.09837207\n",
      "Iteration 425, loss = 0.09823962\n",
      "Iteration 426, loss = 0.09812871\n",
      "Iteration 427, loss = 0.09803064\n",
      "Iteration 428, loss = 0.09792779\n",
      "Iteration 429, loss = 0.09776909\n",
      "Iteration 430, loss = 0.09770339\n",
      "Iteration 431, loss = 0.09756956\n",
      "Iteration 432, loss = 0.09757885\n",
      "Iteration 433, loss = 0.09740910\n",
      "Iteration 434, loss = 0.09724644\n",
      "Iteration 435, loss = 0.09720665\n",
      "Iteration 436, loss = 0.09699020\n",
      "Iteration 437, loss = 0.09701723\n",
      "Iteration 438, loss = 0.09677119\n",
      "Iteration 439, loss = 0.09675657\n",
      "Iteration 440, loss = 0.09669970\n",
      "Iteration 441, loss = 0.09645360\n",
      "Iteration 442, loss = 0.09633381\n",
      "Iteration 443, loss = 0.09631776\n",
      "Iteration 444, loss = 0.09621693\n",
      "Iteration 445, loss = 0.09611462\n",
      "Iteration 446, loss = 0.09584343\n",
      "Iteration 447, loss = 0.09575135\n",
      "Iteration 448, loss = 0.09567415\n",
      "Iteration 449, loss = 0.09556309\n",
      "Iteration 450, loss = 0.09541951\n",
      "Iteration 451, loss = 0.09538362\n",
      "Iteration 452, loss = 0.09523576\n",
      "Iteration 453, loss = 0.09507886\n",
      "Iteration 454, loss = 0.09514126\n",
      "Iteration 455, loss = 0.09493440\n",
      "Iteration 456, loss = 0.09477909\n",
      "Iteration 457, loss = 0.09475164\n",
      "Iteration 458, loss = 0.09464545\n",
      "Iteration 459, loss = 0.09447526\n",
      "Iteration 460, loss = 0.09435918\n",
      "Iteration 461, loss = 0.09431029\n",
      "Iteration 462, loss = 0.09416448\n",
      "Iteration 463, loss = 0.09405287\n",
      "Iteration 464, loss = 0.09397478\n",
      "Iteration 465, loss = 0.09380619\n",
      "Iteration 466, loss = 0.09382553\n",
      "Iteration 467, loss = 0.09364020\n",
      "Iteration 468, loss = 0.09360837\n",
      "Iteration 469, loss = 0.09347599\n",
      "Iteration 470, loss = 0.09328747\n",
      "Iteration 471, loss = 0.09321320\n",
      "Iteration 472, loss = 0.09308093\n",
      "Iteration 473, loss = 0.09301368\n",
      "Iteration 474, loss = 0.09285144\n",
      "Iteration 475, loss = 0.09279255\n",
      "Iteration 476, loss = 0.09279238\n",
      "Iteration 477, loss = 0.09259154\n",
      "Iteration 478, loss = 0.09256480\n",
      "Iteration 479, loss = 0.09235860\n",
      "Iteration 480, loss = 0.09235335\n",
      "Iteration 481, loss = 0.09215399\n",
      "Iteration 482, loss = 0.09207411\n",
      "Iteration 483, loss = 0.09190500\n",
      "Iteration 484, loss = 0.09182855\n",
      "Iteration 485, loss = 0.09183573\n",
      "Iteration 486, loss = 0.09172925\n",
      "Iteration 487, loss = 0.09151952\n",
      "Iteration 488, loss = 0.09154167\n",
      "Iteration 489, loss = 0.09141204\n",
      "Iteration 490, loss = 0.09123893\n",
      "Iteration 491, loss = 0.09109052\n",
      "Iteration 492, loss = 0.09102654\n",
      "Iteration 493, loss = 0.09091447\n",
      "Iteration 494, loss = 0.09097143\n",
      "Iteration 495, loss = 0.09070528\n",
      "Iteration 496, loss = 0.09067318\n",
      "Iteration 497, loss = 0.09054855\n",
      "Iteration 498, loss = 0.09043233\n",
      "Iteration 499, loss = 0.09040020\n",
      "Iteration 500, loss = 0.09029909\n",
      "Iteration 501, loss = 0.09023852\n",
      "Iteration 502, loss = 0.09006073\n",
      "Iteration 503, loss = 0.08988303\n",
      "Iteration 504, loss = 0.08995559\n",
      "Iteration 505, loss = 0.08986704\n",
      "Iteration 506, loss = 0.08965096\n",
      "Iteration 507, loss = 0.08955920\n",
      "Iteration 508, loss = 0.08945087\n",
      "Iteration 509, loss = 0.08943480\n",
      "Iteration 510, loss = 0.08925128\n",
      "Iteration 511, loss = 0.08915570\n",
      "Iteration 512, loss = 0.08915393\n",
      "Iteration 513, loss = 0.08904516\n",
      "Iteration 514, loss = 0.08892826\n",
      "Iteration 515, loss = 0.08882093\n",
      "Iteration 516, loss = 0.08869882\n",
      "Iteration 517, loss = 0.08863350\n",
      "Iteration 518, loss = 0.08852808\n",
      "Iteration 519, loss = 0.08851312\n",
      "Iteration 520, loss = 0.08832820\n",
      "Iteration 521, loss = 0.08833151\n",
      "Iteration 522, loss = 0.08818931\n",
      "Iteration 523, loss = 0.08813168\n",
      "Iteration 524, loss = 0.08795594\n",
      "Iteration 525, loss = 0.08797481\n",
      "Iteration 526, loss = 0.08778689\n",
      "Iteration 527, loss = 0.08775875\n",
      "Iteration 528, loss = 0.08764079\n",
      "Iteration 529, loss = 0.08758913\n",
      "Iteration 530, loss = 0.08748900\n",
      "Iteration 531, loss = 0.08734963\n",
      "Iteration 532, loss = 0.08723096\n",
      "Iteration 533, loss = 0.08717108\n",
      "Iteration 534, loss = 0.08712120\n",
      "Iteration 535, loss = 0.08701075\n",
      "Iteration 536, loss = 0.08700579\n",
      "Iteration 537, loss = 0.08682019\n",
      "Iteration 538, loss = 0.08675588\n",
      "Iteration 539, loss = 0.08673630\n",
      "Iteration 540, loss = 0.08657037\n",
      "Iteration 541, loss = 0.08646143\n",
      "Iteration 542, loss = 0.08641467\n",
      "Iteration 543, loss = 0.08637955\n",
      "Iteration 544, loss = 0.08630965\n",
      "Iteration 545, loss = 0.08611625\n",
      "Iteration 546, loss = 0.08607624\n",
      "Iteration 547, loss = 0.08591382\n",
      "Iteration 548, loss = 0.08582530\n",
      "Iteration 549, loss = 0.08574086\n",
      "Iteration 550, loss = 0.08566639\n",
      "Iteration 551, loss = 0.08560832\n",
      "Iteration 552, loss = 0.08554451\n",
      "Iteration 553, loss = 0.08543642\n",
      "Iteration 554, loss = 0.08527723\n",
      "Iteration 555, loss = 0.08516922\n",
      "Iteration 556, loss = 0.08507237\n",
      "Iteration 557, loss = 0.08503347\n",
      "Iteration 558, loss = 0.08490795\n",
      "Iteration 559, loss = 0.08485509\n",
      "Iteration 560, loss = 0.08480341\n",
      "Iteration 561, loss = 0.08460845\n",
      "Iteration 562, loss = 0.08458832\n",
      "Iteration 563, loss = 0.08452117\n",
      "Iteration 564, loss = 0.08435032\n",
      "Iteration 565, loss = 0.08429149\n",
      "Iteration 566, loss = 0.08424117\n",
      "Iteration 567, loss = 0.08407073\n",
      "Iteration 568, loss = 0.08405983\n",
      "Iteration 569, loss = 0.08398664\n",
      "Iteration 570, loss = 0.08384119\n",
      "Iteration 571, loss = 0.08374738\n",
      "Iteration 572, loss = 0.08363553\n",
      "Iteration 573, loss = 0.08359655\n",
      "Iteration 574, loss = 0.08350502\n",
      "Iteration 575, loss = 0.08338259\n",
      "Iteration 576, loss = 0.08330515\n",
      "Iteration 577, loss = 0.08322223\n",
      "Iteration 578, loss = 0.08312948\n",
      "Iteration 579, loss = 0.08301293\n",
      "Iteration 580, loss = 0.08294424\n",
      "Iteration 581, loss = 0.08295531\n",
      "Iteration 582, loss = 0.08276006\n",
      "Iteration 583, loss = 0.08270385\n",
      "Iteration 584, loss = 0.08257907\n",
      "Iteration 585, loss = 0.08249712\n",
      "Iteration 586, loss = 0.08239491\n",
      "Iteration 587, loss = 0.08231171\n",
      "Iteration 588, loss = 0.08223683\n",
      "Iteration 589, loss = 0.08218797\n",
      "Iteration 590, loss = 0.08205802\n",
      "Iteration 591, loss = 0.08199903\n",
      "Iteration 592, loss = 0.08198693\n",
      "Iteration 593, loss = 0.08193440\n",
      "Iteration 594, loss = 0.08174699\n",
      "Iteration 595, loss = 0.08163130\n",
      "Iteration 596, loss = 0.08154707\n",
      "Iteration 597, loss = 0.08156405\n",
      "Iteration 598, loss = 0.08139902\n",
      "Iteration 599, loss = 0.08131152\n",
      "Iteration 600, loss = 0.08119080\n",
      "Iteration 601, loss = 0.08117689\n",
      "Iteration 602, loss = 0.08105103\n",
      "Iteration 603, loss = 0.08093127\n",
      "Iteration 604, loss = 0.08088905\n",
      "Iteration 605, loss = 0.08085667\n",
      "Iteration 606, loss = 0.08067794\n",
      "Iteration 607, loss = 0.08065026\n",
      "Iteration 608, loss = 0.08060614\n",
      "Iteration 609, loss = 0.08044202\n",
      "Iteration 610, loss = 0.08044845\n",
      "Iteration 611, loss = 0.08026015\n",
      "Iteration 612, loss = 0.08023070\n",
      "Iteration 613, loss = 0.08023849\n",
      "Iteration 614, loss = 0.08005521\n",
      "Iteration 615, loss = 0.08003994\n",
      "Iteration 616, loss = 0.07984803\n",
      "Iteration 617, loss = 0.07980698\n",
      "Iteration 618, loss = 0.07970574\n",
      "Iteration 619, loss = 0.07959427\n",
      "Iteration 620, loss = 0.07951345\n",
      "Iteration 621, loss = 0.07946141\n",
      "Iteration 622, loss = 0.07940753\n",
      "Iteration 623, loss = 0.07926808\n",
      "Iteration 624, loss = 0.07917444\n",
      "Iteration 625, loss = 0.07908397\n",
      "Iteration 626, loss = 0.07904312\n",
      "Iteration 627, loss = 0.07896566\n",
      "Iteration 628, loss = 0.07889139\n",
      "Iteration 629, loss = 0.07873164\n",
      "Iteration 630, loss = 0.07865177\n",
      "Iteration 631, loss = 0.07861752\n",
      "Iteration 632, loss = 0.07847016\n",
      "Iteration 633, loss = 0.07851213\n",
      "Iteration 634, loss = 0.07835521\n",
      "Iteration 635, loss = 0.07825959\n",
      "Iteration 636, loss = 0.07823783\n",
      "Iteration 637, loss = 0.07806152\n",
      "Iteration 638, loss = 0.07801149\n",
      "Iteration 639, loss = 0.07791597\n",
      "Iteration 640, loss = 0.07781755\n",
      "Iteration 641, loss = 0.07772964\n",
      "Iteration 642, loss = 0.07762883\n",
      "Iteration 643, loss = 0.07756092\n",
      "Iteration 644, loss = 0.07749508\n",
      "Iteration 645, loss = 0.07734840\n",
      "Iteration 646, loss = 0.07735108\n",
      "Iteration 647, loss = 0.07716291\n",
      "Iteration 648, loss = 0.07719081\n",
      "Iteration 649, loss = 0.07698889\n",
      "Iteration 650, loss = 0.07697591\n",
      "Iteration 651, loss = 0.07685185\n",
      "Iteration 652, loss = 0.07677281\n",
      "Iteration 653, loss = 0.07667375\n",
      "Iteration 654, loss = 0.07663886\n",
      "Iteration 655, loss = 0.07650630\n",
      "Iteration 656, loss = 0.07646864\n",
      "Iteration 657, loss = 0.07634051\n",
      "Iteration 658, loss = 0.07630606\n",
      "Iteration 659, loss = 0.07614260\n",
      "Iteration 660, loss = 0.07601107\n",
      "Iteration 661, loss = 0.07595634\n",
      "Iteration 662, loss = 0.07586330\n",
      "Iteration 663, loss = 0.07579222\n",
      "Iteration 664, loss = 0.07571644\n",
      "Iteration 665, loss = 0.07556127\n",
      "Iteration 666, loss = 0.07552153\n",
      "Iteration 667, loss = 0.07541703\n",
      "Iteration 668, loss = 0.07531783\n",
      "Iteration 669, loss = 0.07520256\n",
      "Iteration 670, loss = 0.07520171\n",
      "Iteration 671, loss = 0.07506195\n",
      "Iteration 672, loss = 0.07498854\n",
      "Iteration 673, loss = 0.07484289\n",
      "Iteration 674, loss = 0.07475334\n",
      "Iteration 675, loss = 0.07469645\n",
      "Iteration 676, loss = 0.07459675\n",
      "Iteration 677, loss = 0.07452611\n",
      "Iteration 678, loss = 0.07438891\n",
      "Iteration 679, loss = 0.07436094\n",
      "Iteration 680, loss = 0.07419969\n",
      "Iteration 681, loss = 0.07408721\n",
      "Iteration 682, loss = 0.07400835\n",
      "Iteration 683, loss = 0.07390077\n",
      "Iteration 684, loss = 0.07382705\n",
      "Iteration 685, loss = 0.07379324\n",
      "Iteration 686, loss = 0.07363646\n",
      "Iteration 687, loss = 0.07367232\n",
      "Iteration 688, loss = 0.07348471\n",
      "Iteration 689, loss = 0.07341133\n",
      "Iteration 690, loss = 0.07331377\n",
      "Iteration 691, loss = 0.07321005\n",
      "Iteration 692, loss = 0.07303749\n",
      "Iteration 693, loss = 0.07294234\n",
      "Iteration 694, loss = 0.07283904\n",
      "Iteration 695, loss = 0.07275069\n",
      "Iteration 696, loss = 0.07261892\n",
      "Iteration 697, loss = 0.07259513\n",
      "Iteration 698, loss = 0.07250835\n",
      "Iteration 699, loss = 0.07238203\n",
      "Iteration 700, loss = 0.07230960\n",
      "Iteration 701, loss = 0.07224584\n",
      "Iteration 702, loss = 0.07212024\n",
      "Iteration 703, loss = 0.07194371\n",
      "Iteration 704, loss = 0.07188096\n",
      "Iteration 705, loss = 0.07175411\n",
      "Iteration 706, loss = 0.07164533\n",
      "Iteration 707, loss = 0.07153630\n",
      "Iteration 708, loss = 0.07146525\n",
      "Iteration 709, loss = 0.07139190\n",
      "Iteration 710, loss = 0.07124731\n",
      "Iteration 711, loss = 0.07119334\n",
      "Iteration 712, loss = 0.07101390\n",
      "Iteration 713, loss = 0.07089715\n",
      "Iteration 714, loss = 0.07084204\n",
      "Iteration 715, loss = 0.07079452\n",
      "Iteration 716, loss = 0.07057949\n",
      "Iteration 717, loss = 0.07055948\n",
      "Iteration 718, loss = 0.07046619\n",
      "Iteration 719, loss = 0.07024170\n",
      "Iteration 720, loss = 0.07022321\n",
      "Iteration 721, loss = 0.07009677\n",
      "Iteration 722, loss = 0.06997967\n",
      "Iteration 723, loss = 0.06983103\n",
      "Iteration 724, loss = 0.06977453\n",
      "Iteration 725, loss = 0.06962227\n",
      "Iteration 726, loss = 0.06949661\n",
      "Iteration 727, loss = 0.06944166\n",
      "Iteration 728, loss = 0.06934734\n",
      "Iteration 729, loss = 0.06923264\n",
      "Iteration 730, loss = 0.06907908\n",
      "Iteration 731, loss = 0.06897710\n",
      "Iteration 732, loss = 0.06891708\n",
      "Iteration 733, loss = 0.06870869\n",
      "Iteration 734, loss = 0.06859984\n",
      "Iteration 735, loss = 0.06847744\n",
      "Iteration 736, loss = 0.06835440\n",
      "Iteration 737, loss = 0.06825671\n",
      "Iteration 738, loss = 0.06816871\n",
      "Iteration 739, loss = 0.06799439\n",
      "Iteration 740, loss = 0.06799695\n",
      "Iteration 741, loss = 0.06781946\n",
      "Iteration 742, loss = 0.06776080\n",
      "Iteration 743, loss = 0.06760304\n",
      "Iteration 744, loss = 0.06743368\n",
      "Iteration 745, loss = 0.06732969\n",
      "Iteration 746, loss = 0.06721807\n",
      "Iteration 747, loss = 0.06712127\n",
      "Iteration 748, loss = 0.06694599\n",
      "Iteration 749, loss = 0.06686554\n",
      "Iteration 750, loss = 0.06670123\n",
      "Iteration 751, loss = 0.06658389\n",
      "Iteration 752, loss = 0.06649142\n",
      "Iteration 753, loss = 0.06633260\n",
      "Iteration 754, loss = 0.06621256\n",
      "Iteration 755, loss = 0.06610343\n",
      "Iteration 756, loss = 0.06599645\n",
      "Iteration 757, loss = 0.06587655\n",
      "Iteration 758, loss = 0.06577088\n",
      "Iteration 759, loss = 0.06560305\n",
      "Iteration 760, loss = 0.06549320\n",
      "Iteration 761, loss = 0.06534851\n",
      "Iteration 762, loss = 0.06528214\n",
      "Iteration 763, loss = 0.06511696\n",
      "Iteration 764, loss = 0.06499050\n",
      "Iteration 765, loss = 0.06484513\n",
      "Iteration 766, loss = 0.06476051\n",
      "Iteration 767, loss = 0.06460656\n",
      "Iteration 768, loss = 0.06454522\n",
      "Iteration 769, loss = 0.06433668\n",
      "Iteration 770, loss = 0.06430778\n",
      "Iteration 771, loss = 0.06407107\n",
      "Iteration 772, loss = 0.06402918\n",
      "Iteration 773, loss = 0.06387410\n",
      "Iteration 774, loss = 0.06371358\n",
      "Iteration 775, loss = 0.06353535\n",
      "Iteration 776, loss = 0.06339314\n",
      "Iteration 777, loss = 0.06326445\n",
      "Iteration 778, loss = 0.06318110\n",
      "Iteration 779, loss = 0.06299015\n",
      "Iteration 780, loss = 0.06285773\n",
      "Iteration 781, loss = 0.06273423\n",
      "Iteration 782, loss = 0.06255730\n",
      "Iteration 783, loss = 0.06248217\n",
      "Iteration 784, loss = 0.06234397\n",
      "Iteration 785, loss = 0.06228604\n",
      "Iteration 786, loss = 0.06205907\n",
      "Iteration 787, loss = 0.06197482\n",
      "Iteration 788, loss = 0.06178062\n",
      "Iteration 789, loss = 0.06169751\n",
      "Iteration 790, loss = 0.06147385\n",
      "Iteration 791, loss = 0.06144208\n",
      "Iteration 792, loss = 0.06123035\n",
      "Iteration 793, loss = 0.06106898\n",
      "Iteration 794, loss = 0.06098414\n",
      "Iteration 795, loss = 0.06083614\n",
      "Iteration 796, loss = 0.06070261\n",
      "Iteration 797, loss = 0.06047549\n",
      "Iteration 798, loss = 0.06036031\n",
      "Iteration 799, loss = 0.06020848\n",
      "Iteration 800, loss = 0.06016786\n",
      "Iteration 801, loss = 0.05988918\n",
      "Iteration 802, loss = 0.05973913\n",
      "Iteration 803, loss = 0.05975783\n",
      "Iteration 804, loss = 0.05948697\n",
      "Iteration 805, loss = 0.05930115\n",
      "Iteration 806, loss = 0.05915022\n",
      "Iteration 807, loss = 0.05904392\n",
      "Iteration 808, loss = 0.05885056\n",
      "Iteration 809, loss = 0.05870602\n",
      "Iteration 810, loss = 0.05860072\n",
      "Iteration 811, loss = 0.05841473\n",
      "Iteration 812, loss = 0.05829920\n",
      "Iteration 813, loss = 0.05813006\n",
      "Iteration 814, loss = 0.05795469\n",
      "Iteration 815, loss = 0.05779153\n",
      "Iteration 816, loss = 0.05764728\n",
      "Iteration 817, loss = 0.05763170\n",
      "Iteration 818, loss = 0.05736107\n",
      "Iteration 819, loss = 0.05716674\n",
      "Iteration 820, loss = 0.05707057\n",
      "Iteration 821, loss = 0.05686288\n",
      "Iteration 822, loss = 0.05680407\n",
      "Iteration 823, loss = 0.05652832\n",
      "Iteration 824, loss = 0.05644682\n",
      "Iteration 825, loss = 0.05621928\n",
      "Iteration 826, loss = 0.05611322\n",
      "Iteration 827, loss = 0.05588851\n",
      "Iteration 828, loss = 0.05574438\n",
      "Iteration 829, loss = 0.05560705\n",
      "Iteration 830, loss = 0.05539186\n",
      "Iteration 831, loss = 0.05537996\n",
      "Iteration 832, loss = 0.05509843\n",
      "Iteration 833, loss = 0.05497549\n",
      "Iteration 834, loss = 0.05480517\n",
      "Iteration 835, loss = 0.05461744\n",
      "Iteration 836, loss = 0.05442245\n",
      "Iteration 837, loss = 0.05439385\n",
      "Iteration 838, loss = 0.05418818\n",
      "Iteration 839, loss = 0.05392643\n",
      "Iteration 840, loss = 0.05377024\n",
      "Iteration 841, loss = 0.05364672\n",
      "Iteration 842, loss = 0.05348603\n",
      "Iteration 843, loss = 0.05328830\n",
      "Iteration 844, loss = 0.05312460\n",
      "Iteration 845, loss = 0.05303176\n",
      "Iteration 846, loss = 0.05278971\n",
      "Iteration 847, loss = 0.05261794\n",
      "Iteration 848, loss = 0.05237207\n",
      "Iteration 849, loss = 0.05227650\n",
      "Iteration 850, loss = 0.05211440\n",
      "Iteration 851, loss = 0.05191981\n",
      "Iteration 852, loss = 0.05176416\n",
      "Iteration 853, loss = 0.05158717\n",
      "Iteration 854, loss = 0.05141066\n",
      "Iteration 855, loss = 0.05123177\n",
      "Iteration 856, loss = 0.05104853\n",
      "Iteration 857, loss = 0.05086612\n",
      "Iteration 858, loss = 0.05071723\n",
      "Iteration 859, loss = 0.05062897\n",
      "Iteration 860, loss = 0.05038150\n",
      "Iteration 861, loss = 0.05013236\n",
      "Iteration 862, loss = 0.05007536\n",
      "Iteration 863, loss = 0.04988613\n",
      "Iteration 864, loss = 0.04963699\n",
      "Iteration 865, loss = 0.04945484\n",
      "Iteration 866, loss = 0.04929442\n",
      "Iteration 867, loss = 0.04907807\n",
      "Iteration 868, loss = 0.04894107\n",
      "Iteration 869, loss = 0.04876199\n",
      "Iteration 870, loss = 0.04859449\n",
      "Iteration 871, loss = 0.04837908\n",
      "Iteration 872, loss = 0.04817671\n",
      "Iteration 873, loss = 0.04800953\n",
      "Iteration 874, loss = 0.04787523\n",
      "Iteration 875, loss = 0.04772948\n",
      "Iteration 876, loss = 0.04746684\n",
      "Iteration 877, loss = 0.04732372\n",
      "Iteration 878, loss = 0.04712712\n",
      "Iteration 879, loss = 0.04692790\n",
      "Iteration 880, loss = 0.04671432\n",
      "Iteration 881, loss = 0.04664011\n",
      "Iteration 882, loss = 0.04634761\n",
      "Iteration 883, loss = 0.04621705\n",
      "Iteration 884, loss = 0.04604272\n",
      "Iteration 885, loss = 0.04583377\n",
      "Iteration 886, loss = 0.04571506\n",
      "Iteration 887, loss = 0.04542570\n",
      "Iteration 888, loss = 0.04527714\n",
      "Iteration 889, loss = 0.04511537\n",
      "Iteration 890, loss = 0.04493135\n",
      "Iteration 891, loss = 0.04466690\n",
      "Iteration 892, loss = 0.04451776\n",
      "Iteration 893, loss = 0.04435756\n",
      "Iteration 894, loss = 0.04412675\n",
      "Iteration 895, loss = 0.04396698\n",
      "Iteration 896, loss = 0.04376513\n",
      "Iteration 897, loss = 0.04361515\n",
      "Iteration 898, loss = 0.04339073\n",
      "Iteration 899, loss = 0.04318600\n",
      "Iteration 900, loss = 0.04296473\n",
      "Iteration 901, loss = 0.04283352\n",
      "Iteration 902, loss = 0.04268294\n",
      "Iteration 903, loss = 0.04247849\n",
      "Iteration 904, loss = 0.04226719\n",
      "Iteration 905, loss = 0.04202793\n",
      "Iteration 906, loss = 0.04190415\n",
      "Iteration 907, loss = 0.04176229\n",
      "Iteration 908, loss = 0.04149909\n",
      "Iteration 909, loss = 0.04130205\n",
      "Iteration 910, loss = 0.04108559\n",
      "Iteration 911, loss = 0.04091877\n",
      "Iteration 912, loss = 0.04069682\n",
      "Iteration 913, loss = 0.04062653\n",
      "Iteration 914, loss = 0.04035455\n",
      "Iteration 915, loss = 0.04017344\n",
      "Iteration 916, loss = 0.04000816\n",
      "Iteration 917, loss = 0.03977117\n",
      "Iteration 918, loss = 0.03957131\n",
      "Iteration 919, loss = 0.03942261\n",
      "Iteration 920, loss = 0.03922318\n",
      "Iteration 921, loss = 0.03895717\n",
      "Iteration 922, loss = 0.03880542\n",
      "Iteration 923, loss = 0.03864041\n",
      "Iteration 924, loss = 0.03843330\n",
      "Iteration 925, loss = 0.03820050\n",
      "Iteration 926, loss = 0.03796058\n",
      "Iteration 927, loss = 0.03779392\n",
      "Iteration 928, loss = 0.03768657\n",
      "Iteration 929, loss = 0.03744879\n",
      "Iteration 930, loss = 0.03739293\n",
      "Iteration 931, loss = 0.03710932\n",
      "Iteration 932, loss = 0.03684786\n",
      "Iteration 933, loss = 0.03668342\n",
      "Iteration 934, loss = 0.03644393\n",
      "Iteration 935, loss = 0.03627139\n",
      "Iteration 936, loss = 0.03610006\n",
      "Iteration 937, loss = 0.03586574\n",
      "Iteration 938, loss = 0.03567009\n",
      "Iteration 939, loss = 0.03551727\n",
      "Iteration 940, loss = 0.03530217\n",
      "Iteration 941, loss = 0.03508110\n",
      "Iteration 942, loss = 0.03500924\n",
      "Iteration 943, loss = 0.03473564\n",
      "Iteration 944, loss = 0.03460022\n",
      "Iteration 945, loss = 0.03438551\n",
      "Iteration 946, loss = 0.03418683\n",
      "Iteration 947, loss = 0.03393057\n",
      "Iteration 948, loss = 0.03378885\n",
      "Iteration 949, loss = 0.03353045\n",
      "Iteration 950, loss = 0.03341175\n",
      "Iteration 951, loss = 0.03318317\n",
      "Iteration 952, loss = 0.03293726\n",
      "Iteration 953, loss = 0.03279025\n",
      "Iteration 954, loss = 0.03257778\n",
      "Iteration 955, loss = 0.03243108\n",
      "Iteration 956, loss = 0.03216278\n",
      "Iteration 957, loss = 0.03197069\n",
      "Iteration 958, loss = 0.03190298\n",
      "Iteration 959, loss = 0.03163950\n",
      "Iteration 960, loss = 0.03150308\n",
      "Iteration 961, loss = 0.03129626\n",
      "Iteration 962, loss = 0.03106471\n",
      "Iteration 963, loss = 0.03088806\n",
      "Iteration 964, loss = 0.03066877\n",
      "Iteration 965, loss = 0.03051696\n",
      "Iteration 966, loss = 0.03028625\n",
      "Iteration 967, loss = 0.03006032\n",
      "Iteration 968, loss = 0.02994108\n",
      "Iteration 969, loss = 0.02973752\n",
      "Iteration 970, loss = 0.02948790\n",
      "Iteration 971, loss = 0.02934822\n",
      "Iteration 972, loss = 0.02912991\n",
      "Iteration 973, loss = 0.02898576\n",
      "Iteration 974, loss = 0.02881675\n",
      "Iteration 975, loss = 0.02859448\n",
      "Iteration 976, loss = 0.02835398\n",
      "Iteration 977, loss = 0.02821235\n",
      "Iteration 978, loss = 0.02800755\n",
      "Iteration 979, loss = 0.02783334\n",
      "Iteration 980, loss = 0.02764578\n",
      "Iteration 981, loss = 0.02749515\n",
      "Iteration 982, loss = 0.02723032\n",
      "Iteration 983, loss = 0.02707165\n",
      "Iteration 984, loss = 0.02688688\n",
      "Iteration 985, loss = 0.02669336\n",
      "Iteration 986, loss = 0.02650218\n",
      "Iteration 987, loss = 0.02630865\n",
      "Iteration 988, loss = 0.02620439\n",
      "Iteration 989, loss = 0.02599058\n",
      "Iteration 990, loss = 0.02583331\n",
      "Iteration 991, loss = 0.02559985\n",
      "Iteration 992, loss = 0.02542104\n",
      "Iteration 993, loss = 0.02522733\n",
      "Iteration 994, loss = 0.02502679\n",
      "Iteration 995, loss = 0.02490745\n",
      "Iteration 996, loss = 0.02469555\n",
      "Iteration 997, loss = 0.02446808\n",
      "Iteration 998, loss = 0.02433709\n",
      "Iteration 999, loss = 0.02414741\n",
      "Iteration 1000, loss = 0.02393017\n",
      "Iteration 1001, loss = 0.02378018\n",
      "Iteration 1002, loss = 0.02359277\n",
      "Iteration 1003, loss = 0.02348773\n",
      "Iteration 1004, loss = 0.02325414\n",
      "Iteration 1005, loss = 0.02303213\n",
      "Iteration 1006, loss = 0.02289519\n",
      "Iteration 1007, loss = 0.02269538\n",
      "Iteration 1008, loss = 0.02258379\n",
      "Iteration 1009, loss = 0.02237219\n",
      "Iteration 1010, loss = 0.02220737\n",
      "Iteration 1011, loss = 0.02200924\n",
      "Iteration 1012, loss = 0.02181887\n",
      "Iteration 1013, loss = 0.02161927\n",
      "Iteration 1014, loss = 0.02154637\n",
      "Iteration 1015, loss = 0.02137152\n",
      "Iteration 1016, loss = 0.02118894\n",
      "Iteration 1017, loss = 0.02097590\n",
      "Iteration 1018, loss = 0.02081596\n",
      "Iteration 1019, loss = 0.02062253\n",
      "Iteration 1020, loss = 0.02051877\n",
      "Iteration 1021, loss = 0.02032694\n",
      "Iteration 1022, loss = 0.02010210\n",
      "Iteration 1023, loss = 0.01999010\n",
      "Iteration 1024, loss = 0.01982440\n",
      "Iteration 1025, loss = 0.01964851\n",
      "Iteration 1026, loss = 0.01948193\n",
      "Iteration 1027, loss = 0.01933083\n",
      "Iteration 1028, loss = 0.01918388\n",
      "Iteration 1029, loss = 0.01896098\n",
      "Iteration 1030, loss = 0.01882697\n",
      "Iteration 1031, loss = 0.01867786\n",
      "Iteration 1032, loss = 0.01849507\n",
      "Iteration 1033, loss = 0.01832335\n",
      "Iteration 1034, loss = 0.01819364\n",
      "Iteration 1035, loss = 0.01802074\n",
      "Iteration 1036, loss = 0.01791416\n",
      "Iteration 1037, loss = 0.01773947\n",
      "Iteration 1038, loss = 0.01754448\n",
      "Iteration 1039, loss = 0.01735380\n",
      "Iteration 1040, loss = 0.01722750\n",
      "Iteration 1041, loss = 0.01707568\n",
      "Iteration 1042, loss = 0.01690338\n",
      "Iteration 1043, loss = 0.01682627\n",
      "Iteration 1044, loss = 0.01663819\n",
      "Iteration 1045, loss = 0.01645471\n",
      "Iteration 1046, loss = 0.01630453\n",
      "Iteration 1047, loss = 0.01616906\n",
      "Iteration 1048, loss = 0.01604032\n",
      "Iteration 1049, loss = 0.01588178\n",
      "Iteration 1050, loss = 0.01571213\n",
      "Iteration 1051, loss = 0.01554886\n",
      "Iteration 1052, loss = 0.01541896\n",
      "Iteration 1053, loss = 0.01522631\n",
      "Iteration 1054, loss = 0.01513821\n",
      "Iteration 1055, loss = 0.01498408\n",
      "Iteration 1056, loss = 0.01480611\n",
      "Iteration 1057, loss = 0.01466198\n",
      "Iteration 1058, loss = 0.01454428\n",
      "Iteration 1059, loss = 0.01438597\n",
      "Iteration 1060, loss = 0.01430810\n",
      "Iteration 1061, loss = 0.01410911\n",
      "Iteration 1062, loss = 0.01396962\n",
      "Iteration 1063, loss = 0.01386951\n",
      "Iteration 1064, loss = 0.01370320\n",
      "Iteration 1065, loss = 0.01360643\n",
      "Iteration 1066, loss = 0.01339813\n",
      "Iteration 1067, loss = 0.01333560\n",
      "Iteration 1068, loss = 0.01313023\n",
      "Iteration 1069, loss = 0.01303414\n",
      "Iteration 1070, loss = 0.01287361\n",
      "Iteration 1071, loss = 0.01277177\n",
      "Iteration 1072, loss = 0.01263142\n",
      "Iteration 1073, loss = 0.01253496\n",
      "Iteration 1074, loss = 0.01241403\n",
      "Iteration 1075, loss = 0.01225943\n",
      "Iteration 1076, loss = 0.01211712\n",
      "Iteration 1077, loss = 0.01196162\n",
      "Iteration 1078, loss = 0.01187623\n",
      "Iteration 1079, loss = 0.01178578\n",
      "Iteration 1080, loss = 0.01161033\n",
      "Iteration 1081, loss = 0.01150077\n",
      "Iteration 1082, loss = 0.01138008\n",
      "Iteration 1083, loss = 0.01124592\n",
      "Iteration 1084, loss = 0.01113528\n",
      "Iteration 1085, loss = 0.01100443\n",
      "Iteration 1086, loss = 0.01090584\n",
      "Iteration 1087, loss = 0.01075837\n",
      "Iteration 1088, loss = 0.01063895\n",
      "Iteration 1089, loss = 0.01054663\n",
      "Iteration 1090, loss = 0.01038854\n",
      "Iteration 1091, loss = 0.01029613\n",
      "Iteration 1092, loss = 0.01017687\n",
      "Iteration 1093, loss = 0.01007817\n",
      "Iteration 1094, loss = 0.00999444\n",
      "Iteration 1095, loss = 0.00982870\n",
      "Iteration 1096, loss = 0.00976299\n",
      "Iteration 1097, loss = 0.00962990\n",
      "Iteration 1098, loss = 0.00948977\n",
      "Iteration 1099, loss = 0.00943710\n",
      "Iteration 1100, loss = 0.00928566\n",
      "Iteration 1101, loss = 0.00923716\n",
      "Iteration 1102, loss = 0.00910987\n",
      "Iteration 1103, loss = 0.00896824\n",
      "Iteration 1104, loss = 0.00887437\n",
      "Iteration 1105, loss = 0.00878105\n",
      "Iteration 1106, loss = 0.00868816\n",
      "Iteration 1107, loss = 0.00860521\n",
      "Iteration 1108, loss = 0.00849415\n",
      "Iteration 1109, loss = 0.00839161\n",
      "Iteration 1110, loss = 0.00826757\n",
      "Iteration 1111, loss = 0.00815981\n",
      "Iteration 1112, loss = 0.00809938\n",
      "Iteration 1113, loss = 0.00800514\n",
      "Iteration 1114, loss = 0.00790127\n",
      "Iteration 1115, loss = 0.00779470\n",
      "Iteration 1116, loss = 0.00768153\n",
      "Iteration 1117, loss = 0.00761601\n",
      "Iteration 1118, loss = 0.00752677\n",
      "Iteration 1119, loss = 0.00740365\n",
      "Iteration 1120, loss = 0.00735102\n",
      "Iteration 1121, loss = 0.00726120\n",
      "Iteration 1122, loss = 0.00713899\n",
      "Iteration 1123, loss = 0.00708208\n",
      "Iteration 1124, loss = 0.00700388\n",
      "Iteration 1125, loss = 0.00688771\n",
      "Iteration 1126, loss = 0.00681471\n",
      "Iteration 1127, loss = 0.00674473\n",
      "Iteration 1128, loss = 0.00666824\n",
      "Iteration 1129, loss = 0.00658014\n",
      "Iteration 1130, loss = 0.00649128\n",
      "Iteration 1131, loss = 0.00640289\n",
      "Iteration 1132, loss = 0.00634195\n",
      "Iteration 1133, loss = 0.00625058\n",
      "Iteration 1134, loss = 0.00617312\n",
      "Iteration 1135, loss = 0.00610046\n",
      "Iteration 1136, loss = 0.00601155\n",
      "Iteration 1137, loss = 0.00592568\n",
      "Iteration 1138, loss = 0.00584859\n",
      "Iteration 1139, loss = 0.00578773\n",
      "Iteration 1140, loss = 0.00571838\n",
      "Iteration 1141, loss = 0.00564985\n",
      "Iteration 1142, loss = 0.00557185\n",
      "Iteration 1143, loss = 0.00549601\n",
      "Iteration 1144, loss = 0.00543059\n",
      "Iteration 1145, loss = 0.00537205\n",
      "Iteration 1146, loss = 0.00531007\n",
      "Iteration 1147, loss = 0.00520718\n",
      "Iteration 1148, loss = 0.00516878\n",
      "Iteration 1149, loss = 0.00508692\n",
      "Iteration 1150, loss = 0.00502857\n",
      "Iteration 1151, loss = 0.00495386\n",
      "Iteration 1152, loss = 0.00488939\n",
      "Iteration 1153, loss = 0.00482553\n",
      "Iteration 1154, loss = 0.00476743\n",
      "Iteration 1155, loss = 0.00470681\n",
      "Iteration 1156, loss = 0.00465154\n",
      "Iteration 1157, loss = 0.00457479\n",
      "Iteration 1158, loss = 0.00452408\n",
      "Iteration 1159, loss = 0.00445557\n",
      "Iteration 1160, loss = 0.00439414\n",
      "Iteration 1161, loss = 0.00434079\n",
      "Iteration 1162, loss = 0.00428535\n",
      "Iteration 1163, loss = 0.00422876\n",
      "Iteration 1164, loss = 0.00419274\n",
      "Iteration 1165, loss = 0.00413142\n",
      "Iteration 1166, loss = 0.00407041\n",
      "Iteration 1167, loss = 0.00403413\n",
      "Iteration 1168, loss = 0.00395956\n",
      "Iteration 1169, loss = 0.00390635\n",
      "Iteration 1170, loss = 0.00385206\n",
      "Iteration 1171, loss = 0.00380717\n",
      "Iteration 1172, loss = 0.00376826\n",
      "Iteration 1173, loss = 0.00370787\n",
      "Iteration 1174, loss = 0.00366812\n",
      "Iteration 1175, loss = 0.00361473\n",
      "Iteration 1176, loss = 0.00356582\n",
      "Iteration 1177, loss = 0.00351734\n",
      "Iteration 1178, loss = 0.00348674\n",
      "Iteration 1179, loss = 0.00343497\n",
      "Iteration 1180, loss = 0.00336755\n",
      "Iteration 1181, loss = 0.00333251\n",
      "Iteration 1182, loss = 0.00328500\n",
      "Iteration 1183, loss = 0.00327054\n",
      "Iteration 1184, loss = 0.00319929\n",
      "Iteration 1185, loss = 0.00317067\n",
      "Iteration 1186, loss = 0.00311710\n",
      "Iteration 1187, loss = 0.00308941\n",
      "Iteration 1188, loss = 0.00305434\n",
      "Iteration 1189, loss = 0.00299488\n",
      "Iteration 1190, loss = 0.00296329\n",
      "Iteration 1191, loss = 0.00292395\n",
      "Iteration 1192, loss = 0.00289894\n",
      "Iteration 1193, loss = 0.00285232\n",
      "Iteration 1194, loss = 0.00281639\n",
      "Iteration 1195, loss = 0.00277743\n",
      "Iteration 1196, loss = 0.00272755\n",
      "Iteration 1197, loss = 0.00269993\n",
      "Iteration 1198, loss = 0.00266693\n",
      "Iteration 1199, loss = 0.00264918\n",
      "Iteration 1200, loss = 0.00260110\n",
      "Iteration 1201, loss = 0.00257928\n",
      "Iteration 1202, loss = 0.00253766\n",
      "Iteration 1203, loss = 0.00250339\n",
      "Iteration 1204, loss = 0.00246812\n",
      "Iteration 1205, loss = 0.00244163\n",
      "Iteration 1206, loss = 0.00242100\n",
      "Iteration 1207, loss = 0.00238048\n",
      "Iteration 1208, loss = 0.00234674\n",
      "Iteration 1209, loss = 0.00232831\n",
      "Iteration 1210, loss = 0.00228667\n",
      "Iteration 1211, loss = 0.00227053\n",
      "Iteration 1212, loss = 0.00223929\n",
      "Iteration 1213, loss = 0.00220477\n",
      "Iteration 1214, loss = 0.00217671\n",
      "Iteration 1215, loss = 0.00214911\n",
      "Iteration 1216, loss = 0.00212088\n",
      "Iteration 1217, loss = 0.00210494\n",
      "Iteration 1218, loss = 0.00208031\n",
      "Iteration 1219, loss = 0.00205089\n",
      "Iteration 1220, loss = 0.00202761\n",
      "Iteration 1221, loss = 0.00199961\n",
      "Iteration 1222, loss = 0.00198469\n",
      "Iteration 1223, loss = 0.00194939\n",
      "Iteration 1224, loss = 0.00193428\n",
      "Iteration 1225, loss = 0.00190413\n",
      "Iteration 1226, loss = 0.00188215\n",
      "Iteration 1227, loss = 0.00185887\n",
      "Iteration 1228, loss = 0.00184163\n",
      "Iteration 1229, loss = 0.00181204\n",
      "Iteration 1230, loss = 0.00179759\n",
      "Iteration 1231, loss = 0.00177297\n",
      "Iteration 1232, loss = 0.00175663\n",
      "Iteration 1233, loss = 0.00172925\n",
      "Iteration 1234, loss = 0.00171956\n",
      "Iteration 1235, loss = 0.00169247\n",
      "Iteration 1236, loss = 0.00167419\n",
      "Iteration 1237, loss = 0.00165828\n",
      "Iteration 1238, loss = 0.00163534\n",
      "Iteration 1239, loss = 0.00161427\n",
      "Iteration 1240, loss = 0.00159651\n",
      "Iteration 1241, loss = 0.00157818\n",
      "Iteration 1242, loss = 0.00156423\n",
      "Iteration 1243, loss = 0.00154378\n",
      "Iteration 1244, loss = 0.00152828\n",
      "Iteration 1245, loss = 0.00150539\n",
      "Iteration 1246, loss = 0.00149228\n",
      "Iteration 1247, loss = 0.00147337\n",
      "Iteration 1248, loss = 0.00145614\n",
      "Training loss did not improve more than tol=0.000100 for 100 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "model.predict(xs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.07925489,  0.96711517,  1.00003837, -0.00385179])"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "print('score:', model.score(xs, ys)) # outputs 0.5\r\n",
    "print('predictions:', model.predict(xs)) # outputs [0, 0, 0, 0]\r\n",
    "print('expected:', np.array([0, 1, 1, 0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "score: 0.9926224133721674\n",
      "predictions: [ 0.07925489  0.96711517  1.00003837 -0.00385179]\n",
      "expected: [0 1 1 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "model.coefs_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[-2.98506281e-104,  1.32974640e+000, -4.88344819e-014,\n",
       "         -1.14752118e+000],\n",
       "        [-1.85865121e-028, -1.40772356e+000,  9.04573227e-037,\n",
       "          1.04930294e+000]]),\n",
       " array([[ 1.97647357e-050,  4.86886809e-034, -1.64510311e-020,\n",
       "         -8.77519664e-083],\n",
       "        [ 1.28414940e-107,  1.37340420e+000,  3.12454700e-037,\n",
       "         -4.88486742e-049],\n",
       "        [ 3.83898551e-049,  6.80950777e-021,  1.33178064e-077,\n",
       "          5.50951051e-076],\n",
       "        [ 3.51591740e-034,  1.45405995e+000, -3.48279081e-027,\n",
       "         -1.43634840e-021]]),\n",
       " array([[ 5.97207252e-14],\n",
       "        [ 5.81917512e-01],\n",
       "        [ 3.82010564e-63],\n",
       "        [-6.60896067e-65]])]"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "model.intercepts_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([-0.20646505, -0.06404713, -0.16161097,  0.10727908]),\n",
       " array([-0.57187049,  0.03638505, -0.69568369, -0.1366456 ]),\n",
       " array([-0.03269165])]"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "model.coefs_[0].shape[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "interpreter": {
   "hash": "c4a6cc1c2df5ddd62d6925b2a7bdee9abacf912eab37272999970e810b9642fd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}