{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "xs = np.array([\r\n",
    "    0., 0.,\r\n",
    "    0., 1.,\r\n",
    "    1., 0.,\r\n",
    "    1., 1.\r\n",
    "]).reshape(4, 2)\r\n",
    "\r\n",
    "ys = np.array([0., 1., 1., 0.]) #.reshape(4,)\r\n",
    "\r\n",
    "# ys = (ys - np.mean(ys)) / np.std(ys)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "xs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "ys"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = MLPRegressor(activation='relu', max_iter=10000, hidden_layer_sizes=(16,16), random_state=1, verbose=True, early_stopping=False, n_iter_no_change=100, batch_size=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = model.fit(xs, ys)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.82758542\n",
      "Iteration 2, loss = 0.78280590\n",
      "Iteration 3, loss = 0.73179013\n",
      "Iteration 4, loss = 0.68773097\n",
      "Iteration 5, loss = 0.64935881\n",
      "Iteration 6, loss = 0.60940846\n",
      "Iteration 7, loss = 0.58017629\n",
      "Iteration 8, loss = 0.54799949\n",
      "Iteration 9, loss = 0.51092306\n",
      "Iteration 10, loss = 0.48130181\n",
      "Iteration 11, loss = 0.45389832\n",
      "Iteration 12, loss = 0.42405886\n",
      "Iteration 13, loss = 0.40453332\n",
      "Iteration 14, loss = 0.38306742\n",
      "Iteration 15, loss = 0.36279096\n",
      "Iteration 16, loss = 0.34101246\n",
      "Iteration 17, loss = 0.33143932\n",
      "Iteration 18, loss = 0.30916757\n",
      "Iteration 19, loss = 0.29529887\n",
      "Iteration 20, loss = 0.27969379\n",
      "Iteration 21, loss = 0.26833650\n",
      "Iteration 22, loss = 0.25369780\n",
      "Iteration 23, loss = 0.23826733\n",
      "Iteration 24, loss = 0.23089505\n",
      "Iteration 25, loss = 0.22135578\n",
      "Iteration 26, loss = 0.21220122\n",
      "Iteration 27, loss = 0.20022281\n",
      "Iteration 28, loss = 0.19458160\n",
      "Iteration 29, loss = 0.18806167\n",
      "Iteration 30, loss = 0.18360650\n",
      "Iteration 31, loss = 0.17354783\n",
      "Iteration 32, loss = 0.17105632\n",
      "Iteration 33, loss = 0.16716743\n",
      "Iteration 34, loss = 0.15991322\n",
      "Iteration 35, loss = 0.15712191\n",
      "Iteration 36, loss = 0.15371599\n",
      "Iteration 37, loss = 0.15106052\n",
      "Iteration 38, loss = 0.14851991\n",
      "Iteration 39, loss = 0.14577909\n",
      "Iteration 40, loss = 0.14388180\n",
      "Iteration 41, loss = 0.14199967\n",
      "Iteration 42, loss = 0.14161853\n",
      "Iteration 43, loss = 0.14015217\n",
      "Iteration 44, loss = 0.13716056\n",
      "Iteration 45, loss = 0.13560570\n",
      "Iteration 46, loss = 0.13480687\n",
      "Iteration 47, loss = 0.13317961\n",
      "Iteration 48, loss = 0.13326929\n",
      "Iteration 49, loss = 0.13100860\n",
      "Iteration 50, loss = 0.13022105\n",
      "Iteration 51, loss = 0.12975478\n",
      "Iteration 52, loss = 0.12742241\n",
      "Iteration 53, loss = 0.12684431\n",
      "Iteration 54, loss = 0.12569726\n",
      "Iteration 55, loss = 0.12480144\n",
      "Iteration 56, loss = 0.12361001\n",
      "Iteration 57, loss = 0.12300721\n",
      "Iteration 58, loss = 0.12266495\n",
      "Iteration 59, loss = 0.12137665\n",
      "Iteration 60, loss = 0.12001583\n",
      "Iteration 61, loss = 0.11903986\n",
      "Iteration 62, loss = 0.11809869\n",
      "Iteration 63, loss = 0.11819566\n",
      "Iteration 64, loss = 0.11682429\n",
      "Iteration 65, loss = 0.11619717\n",
      "Iteration 66, loss = 0.11532243\n",
      "Iteration 67, loss = 0.11427450\n",
      "Iteration 68, loss = 0.11414668\n",
      "Iteration 69, loss = 0.11317690\n",
      "Iteration 70, loss = 0.11241344\n",
      "Iteration 71, loss = 0.11187020\n",
      "Iteration 72, loss = 0.11092116\n",
      "Iteration 73, loss = 0.11001721\n",
      "Iteration 74, loss = 0.10931140\n",
      "Iteration 75, loss = 0.10866271\n",
      "Iteration 76, loss = 0.10805329\n",
      "Iteration 77, loss = 0.10835211\n",
      "Iteration 78, loss = 0.10683096\n",
      "Iteration 79, loss = 0.10616287\n",
      "Iteration 80, loss = 0.10520403\n",
      "Iteration 81, loss = 0.10506511\n",
      "Iteration 82, loss = 0.10429488\n",
      "Iteration 83, loss = 0.10345306\n",
      "Iteration 84, loss = 0.10286163\n",
      "Iteration 85, loss = 0.10197237\n",
      "Iteration 86, loss = 0.10140317\n",
      "Iteration 87, loss = 0.10148400\n",
      "Iteration 88, loss = 0.10024989\n",
      "Iteration 89, loss = 0.10018449\n",
      "Iteration 90, loss = 0.09888459\n",
      "Iteration 91, loss = 0.09836550\n",
      "Iteration 92, loss = 0.09847772\n",
      "Iteration 93, loss = 0.09726281\n",
      "Iteration 94, loss = 0.09720064\n",
      "Iteration 95, loss = 0.09605679\n",
      "Iteration 96, loss = 0.09570669\n",
      "Iteration 97, loss = 0.09485493\n",
      "Iteration 98, loss = 0.09396430\n",
      "Iteration 99, loss = 0.09317147\n",
      "Iteration 100, loss = 0.09275188\n",
      "Iteration 101, loss = 0.09215441\n",
      "Iteration 102, loss = 0.09124935\n",
      "Iteration 103, loss = 0.09050526\n",
      "Iteration 104, loss = 0.08996030\n",
      "Iteration 105, loss = 0.08927253\n",
      "Iteration 106, loss = 0.08866306\n",
      "Iteration 107, loss = 0.08785434\n",
      "Iteration 108, loss = 0.08723063\n",
      "Iteration 109, loss = 0.08677661\n",
      "Iteration 110, loss = 0.08585510\n",
      "Iteration 111, loss = 0.08540434\n",
      "Iteration 112, loss = 0.08482356\n",
      "Iteration 113, loss = 0.08437957\n",
      "Iteration 114, loss = 0.08411608\n",
      "Iteration 115, loss = 0.08307842\n",
      "Iteration 116, loss = 0.08271390\n",
      "Iteration 117, loss = 0.08179918\n",
      "Iteration 118, loss = 0.08120941\n",
      "Iteration 119, loss = 0.08030864\n",
      "Iteration 120, loss = 0.07971317\n",
      "Iteration 121, loss = 0.07892338\n",
      "Iteration 122, loss = 0.07835470\n",
      "Iteration 123, loss = 0.07781946\n",
      "Iteration 124, loss = 0.07785967\n",
      "Iteration 125, loss = 0.07640959\n",
      "Iteration 126, loss = 0.07595994\n",
      "Iteration 127, loss = 0.07509979\n",
      "Iteration 128, loss = 0.07443833\n",
      "Iteration 129, loss = 0.07417377\n",
      "Iteration 130, loss = 0.07355229\n",
      "Iteration 131, loss = 0.07252734\n",
      "Iteration 132, loss = 0.07186586\n",
      "Iteration 133, loss = 0.07167033\n",
      "Iteration 134, loss = 0.07087966\n",
      "Iteration 135, loss = 0.06991943\n",
      "Iteration 136, loss = 0.06951263\n",
      "Iteration 137, loss = 0.06856142\n",
      "Iteration 138, loss = 0.06786326\n",
      "Iteration 139, loss = 0.06773600\n",
      "Iteration 140, loss = 0.06681047\n",
      "Iteration 141, loss = 0.06605635\n",
      "Iteration 142, loss = 0.06567242\n",
      "Iteration 143, loss = 0.06517085\n",
      "Iteration 144, loss = 0.06470943\n",
      "Iteration 145, loss = 0.06374543\n",
      "Iteration 146, loss = 0.06312278\n",
      "Iteration 147, loss = 0.06248051\n",
      "Iteration 148, loss = 0.06193645\n",
      "Iteration 149, loss = 0.06149534\n",
      "Iteration 150, loss = 0.06060176\n",
      "Iteration 151, loss = 0.05982536\n",
      "Iteration 152, loss = 0.05928371\n",
      "Iteration 153, loss = 0.05867426\n",
      "Iteration 154, loss = 0.05846172\n",
      "Iteration 155, loss = 0.05779146\n",
      "Iteration 156, loss = 0.05695155\n",
      "Iteration 157, loss = 0.05620816\n",
      "Iteration 158, loss = 0.05592737\n",
      "Iteration 159, loss = 0.05489997\n",
      "Iteration 160, loss = 0.05445314\n",
      "Iteration 161, loss = 0.05362335\n",
      "Iteration 162, loss = 0.05304912\n",
      "Iteration 163, loss = 0.05221126\n",
      "Iteration 164, loss = 0.05168945\n",
      "Iteration 165, loss = 0.05089183\n",
      "Iteration 166, loss = 0.05056226\n",
      "Iteration 167, loss = 0.04984749\n",
      "Iteration 168, loss = 0.04922020\n",
      "Iteration 169, loss = 0.04883112\n",
      "Iteration 170, loss = 0.04783386\n",
      "Iteration 171, loss = 0.04748635\n",
      "Iteration 172, loss = 0.04686730\n",
      "Iteration 173, loss = 0.04629546\n",
      "Iteration 174, loss = 0.04571212\n",
      "Iteration 175, loss = 0.04510265\n",
      "Iteration 176, loss = 0.04479784\n",
      "Iteration 177, loss = 0.04399560\n",
      "Iteration 178, loss = 0.04328569\n",
      "Iteration 179, loss = 0.04316970\n",
      "Iteration 180, loss = 0.04242793\n",
      "Iteration 181, loss = 0.04220119\n",
      "Iteration 182, loss = 0.04118221\n",
      "Iteration 183, loss = 0.04063848\n",
      "Iteration 184, loss = 0.04003423\n",
      "Iteration 185, loss = 0.03974677\n",
      "Iteration 186, loss = 0.03918254\n",
      "Iteration 187, loss = 0.03830270\n",
      "Iteration 188, loss = 0.03785786\n",
      "Iteration 189, loss = 0.03723444\n",
      "Iteration 190, loss = 0.03694656\n",
      "Iteration 191, loss = 0.03623568\n",
      "Iteration 192, loss = 0.03558689\n",
      "Iteration 193, loss = 0.03524074\n",
      "Iteration 194, loss = 0.03460968\n",
      "Iteration 195, loss = 0.03423554\n",
      "Iteration 196, loss = 0.03338961\n",
      "Iteration 197, loss = 0.03316565\n",
      "Iteration 198, loss = 0.03267003\n",
      "Iteration 199, loss = 0.03207246\n",
      "Iteration 200, loss = 0.03190821\n",
      "Iteration 201, loss = 0.03117173\n",
      "Iteration 202, loss = 0.03053125\n",
      "Iteration 203, loss = 0.02998268\n",
      "Iteration 204, loss = 0.02955880\n",
      "Iteration 205, loss = 0.02904935\n",
      "Iteration 206, loss = 0.02851519\n",
      "Iteration 207, loss = 0.02809967\n",
      "Iteration 208, loss = 0.02763205\n",
      "Iteration 209, loss = 0.02726144\n",
      "Iteration 210, loss = 0.02681632\n",
      "Iteration 211, loss = 0.02637380\n",
      "Iteration 212, loss = 0.02593629\n",
      "Iteration 213, loss = 0.02533391\n",
      "Iteration 214, loss = 0.02493396\n",
      "Iteration 215, loss = 0.02458615\n",
      "Iteration 216, loss = 0.02396538\n",
      "Iteration 217, loss = 0.02359414\n",
      "Iteration 218, loss = 0.02330343\n",
      "Iteration 219, loss = 0.02275734\n",
      "Iteration 220, loss = 0.02232091\n",
      "Iteration 221, loss = 0.02196111\n",
      "Iteration 222, loss = 0.02141064\n",
      "Iteration 223, loss = 0.02110087\n",
      "Iteration 224, loss = 0.02062363\n",
      "Iteration 225, loss = 0.02036144\n",
      "Iteration 226, loss = 0.01985136\n",
      "Iteration 227, loss = 0.01950878\n",
      "Iteration 228, loss = 0.01903293\n",
      "Iteration 229, loss = 0.01885587\n",
      "Iteration 230, loss = 0.01822856\n",
      "Iteration 231, loss = 0.01787864\n",
      "Iteration 232, loss = 0.01763362\n",
      "Iteration 233, loss = 0.01725787\n",
      "Iteration 234, loss = 0.01681977\n",
      "Iteration 235, loss = 0.01652405\n",
      "Iteration 236, loss = 0.01599187\n",
      "Iteration 237, loss = 0.01530465\n",
      "Iteration 238, loss = 0.01490073\n",
      "Iteration 239, loss = 0.01471526\n",
      "Iteration 240, loss = 0.01430397\n",
      "Iteration 241, loss = 0.01370826\n",
      "Iteration 242, loss = 0.01330698\n",
      "Iteration 243, loss = 0.01296845\n",
      "Iteration 244, loss = 0.01244050\n",
      "Iteration 245, loss = 0.01201228\n",
      "Iteration 246, loss = 0.01154066\n",
      "Iteration 247, loss = 0.01114936\n",
      "Iteration 248, loss = 0.01068317\n",
      "Iteration 249, loss = 0.01033445\n",
      "Iteration 250, loss = 0.01009491\n",
      "Iteration 251, loss = 0.00981819\n",
      "Iteration 252, loss = 0.00943078\n",
      "Iteration 253, loss = 0.00896057\n",
      "Iteration 254, loss = 0.00877754\n",
      "Iteration 255, loss = 0.00839197\n",
      "Iteration 256, loss = 0.00804228\n",
      "Iteration 257, loss = 0.00770684\n",
      "Iteration 258, loss = 0.00744463\n",
      "Iteration 259, loss = 0.00712041\n",
      "Iteration 260, loss = 0.00697403\n",
      "Iteration 261, loss = 0.00673211\n",
      "Iteration 262, loss = 0.00645245\n",
      "Iteration 263, loss = 0.00618724\n",
      "Iteration 264, loss = 0.00606492\n",
      "Iteration 265, loss = 0.00569702\n",
      "Iteration 266, loss = 0.00554090\n",
      "Iteration 267, loss = 0.00531127\n",
      "Iteration 268, loss = 0.00513778\n",
      "Iteration 269, loss = 0.00494681\n",
      "Iteration 270, loss = 0.00477829\n",
      "Iteration 271, loss = 0.00456577\n",
      "Iteration 272, loss = 0.00443985\n",
      "Iteration 273, loss = 0.00425615\n",
      "Iteration 274, loss = 0.00410485\n",
      "Iteration 275, loss = 0.00392467\n",
      "Iteration 276, loss = 0.00378654\n",
      "Iteration 277, loss = 0.00359829\n",
      "Iteration 278, loss = 0.00346620\n",
      "Iteration 279, loss = 0.00336099\n",
      "Iteration 280, loss = 0.00322596\n",
      "Iteration 281, loss = 0.00312263\n",
      "Iteration 282, loss = 0.00296868\n",
      "Iteration 283, loss = 0.00286041\n",
      "Iteration 284, loss = 0.00278831\n",
      "Iteration 285, loss = 0.00273126\n",
      "Iteration 286, loss = 0.00260744\n",
      "Iteration 287, loss = 0.00247091\n",
      "Iteration 288, loss = 0.00239351\n",
      "Iteration 289, loss = 0.00228999\n",
      "Iteration 290, loss = 0.00223331\n",
      "Iteration 291, loss = 0.00212892\n",
      "Iteration 292, loss = 0.00205193\n",
      "Iteration 293, loss = 0.00199472\n",
      "Iteration 294, loss = 0.00192166\n",
      "Iteration 295, loss = 0.00184799\n",
      "Iteration 296, loss = 0.00179643\n",
      "Iteration 297, loss = 0.00173409\n",
      "Iteration 298, loss = 0.00166133\n",
      "Iteration 299, loss = 0.00160944\n",
      "Iteration 300, loss = 0.00156706\n",
      "Iteration 301, loss = 0.00155160\n",
      "Iteration 302, loss = 0.00149599\n",
      "Iteration 303, loss = 0.00144143\n",
      "Iteration 304, loss = 0.00141159\n",
      "Iteration 305, loss = 0.00136975\n",
      "Iteration 306, loss = 0.00133320\n",
      "Iteration 307, loss = 0.00129618\n",
      "Iteration 308, loss = 0.00125367\n",
      "Iteration 309, loss = 0.00122931\n",
      "Iteration 310, loss = 0.00119582\n",
      "Iteration 311, loss = 0.00117228\n",
      "Iteration 312, loss = 0.00114922\n",
      "Iteration 313, loss = 0.00111915\n",
      "Iteration 314, loss = 0.00109749\n",
      "Iteration 315, loss = 0.00108011\n",
      "Iteration 316, loss = 0.00106141\n",
      "Iteration 317, loss = 0.00103476\n",
      "Iteration 318, loss = 0.00101359\n",
      "Iteration 319, loss = 0.00099787\n",
      "Iteration 320, loss = 0.00098652\n",
      "Iteration 321, loss = 0.00097225\n",
      "Iteration 322, loss = 0.00095286\n",
      "Iteration 323, loss = 0.00094344\n",
      "Iteration 324, loss = 0.00093057\n",
      "Iteration 325, loss = 0.00092167\n",
      "Iteration 326, loss = 0.00091107\n",
      "Iteration 327, loss = 0.00089880\n",
      "Iteration 328, loss = 0.00088892\n",
      "Iteration 329, loss = 0.00088487\n",
      "Iteration 330, loss = 0.00087590\n",
      "Iteration 331, loss = 0.00086671\n",
      "Iteration 332, loss = 0.00085717\n",
      "Iteration 333, loss = 0.00085040\n",
      "Iteration 334, loss = 0.00084489\n",
      "Iteration 335, loss = 0.00083983\n",
      "Iteration 336, loss = 0.00083363\n",
      "Iteration 337, loss = 0.00082932\n",
      "Iteration 338, loss = 0.00082270\n",
      "Iteration 339, loss = 0.00081699\n",
      "Iteration 340, loss = 0.00081369\n",
      "Iteration 341, loss = 0.00080961\n",
      "Iteration 342, loss = 0.00080585\n",
      "Iteration 343, loss = 0.00080327\n",
      "Iteration 344, loss = 0.00079938\n",
      "Iteration 345, loss = 0.00079771\n",
      "Iteration 346, loss = 0.00079530\n",
      "Iteration 347, loss = 0.00079204\n",
      "Iteration 348, loss = 0.00078982\n",
      "Iteration 349, loss = 0.00078814\n",
      "Iteration 350, loss = 0.00078549\n",
      "Iteration 351, loss = 0.00078387\n",
      "Iteration 352, loss = 0.00078219\n",
      "Iteration 353, loss = 0.00078006\n",
      "Iteration 354, loss = 0.00077858\n",
      "Iteration 355, loss = 0.00077695\n",
      "Iteration 356, loss = 0.00077625\n",
      "Iteration 357, loss = 0.00077509\n",
      "Iteration 358, loss = 0.00077387\n",
      "Iteration 359, loss = 0.00077252\n",
      "Iteration 360, loss = 0.00077194\n",
      "Iteration 361, loss = 0.00077091\n",
      "Iteration 362, loss = 0.00077010\n",
      "Iteration 363, loss = 0.00076949\n",
      "Iteration 364, loss = 0.00076877\n",
      "Iteration 365, loss = 0.00076813\n",
      "Iteration 366, loss = 0.00076750\n",
      "Iteration 367, loss = 0.00076695\n",
      "Iteration 368, loss = 0.00076640\n",
      "Iteration 369, loss = 0.00076590\n",
      "Iteration 370, loss = 0.00076554\n",
      "Iteration 371, loss = 0.00076512\n",
      "Iteration 372, loss = 0.00076494\n",
      "Iteration 373, loss = 0.00076441\n",
      "Iteration 374, loss = 0.00076431\n",
      "Iteration 375, loss = 0.00076387\n",
      "Iteration 376, loss = 0.00076361\n",
      "Iteration 377, loss = 0.00076332\n",
      "Iteration 378, loss = 0.00076305\n",
      "Iteration 379, loss = 0.00076295\n",
      "Iteration 380, loss = 0.00076271\n",
      "Iteration 381, loss = 0.00076246\n",
      "Iteration 382, loss = 0.00076227\n",
      "Iteration 383, loss = 0.00076222\n",
      "Iteration 384, loss = 0.00076205\n",
      "Iteration 385, loss = 0.00076185\n",
      "Iteration 386, loss = 0.00076176\n",
      "Iteration 387, loss = 0.00076169\n",
      "Iteration 388, loss = 0.00076150\n",
      "Iteration 389, loss = 0.00076142\n",
      "Iteration 390, loss = 0.00076130\n",
      "Iteration 391, loss = 0.00076122\n",
      "Iteration 392, loss = 0.00076117\n",
      "Training loss did not improve more than tol=0.000100 for 100 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model.predict(xs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.0010646 , 0.99781347, 0.99884986, 0.00216278])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print('score:', model.score(xs, ys)) # outputs 0.5\r\n",
    "print('predictions:', model.predict(xs)) # outputs [0, 0, 0, 0]\r\n",
    "print('expected:', np.array([0, 1, 1, 0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "score: 0.9999880852655716\n",
      "predictions: [0.0010646  0.99781347 0.99884986 0.00216278]\n",
      "expected: [0 1 1 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.coefs_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[-3.25373800e-01,  3.55797566e-01, -5.35381004e-01,\n",
       "         -2.84813673e-15, -2.24395637e-01, -4.20713188e-01,\n",
       "         -8.03305298e-07, -2.60566409e-01, -2.98997627e-02,\n",
       "          5.47610312e-02,  2.99633365e-35,  4.09367955e-01,\n",
       "         -1.82557543e-07,  5.60287782e-01, -6.73172712e-01,\n",
       "          5.96045632e-21],\n",
       "        [-4.56696746e-02,  1.03953469e-01,  3.21682315e-01,\n",
       "         -3.15492676e-07,  3.16188902e-01,  5.56658112e-01,\n",
       "          3.32269347e-18,  2.44507722e-01,  5.22606252e-01,\n",
       "          3.39281214e-01, -1.37713623e-04, -8.42355176e-01,\n",
       "         -2.51120709e-06,  3.88000830e-01, -6.73172682e-01,\n",
       "         -1.60991085e-35]]),\n",
       " array([[-3.51708096e-23, -1.62769776e-01, -1.41540600e-05,\n",
       "          5.22984302e-02, -7.04804721e-02, -2.57198144e-20,\n",
       "         -1.37154281e-01, -2.37707779e-01,  2.30313416e-01,\n",
       "         -7.32893802e-09,  9.21428596e-37,  3.16984008e-01,\n",
       "         -3.88416590e-01, -2.51895347e-36,  1.28016296e-25,\n",
       "         -8.20980778e-02],\n",
       "        [-3.97965503e-06,  1.54548772e-01, -6.63070037e-30,\n",
       "          1.64560163e-01,  1.82141323e-01,  2.02947383e-36,\n",
       "          4.65272883e-01, -4.52708566e-01, -4.64587062e-01,\n",
       "          3.25938958e-11,  1.65810554e-35, -4.30783935e-01,\n",
       "          5.14287725e-01, -5.18160299e-31,  4.22522906e-18,\n",
       "          5.72146218e-02],\n",
       "        [ 8.64475572e-08, -1.90130435e-01,  5.14556376e-18,\n",
       "         -3.83687954e-01,  3.65638046e-02,  2.05972784e-07,\n",
       "         -3.35481535e-01,  6.33144644e-01,  3.67175857e-01,\n",
       "          6.09950620e-34, -9.94396223e-08,  6.41421320e-01,\n",
       "         -2.66035780e-01,  1.08904233e-36,  3.96385852e-36,\n",
       "         -5.27986147e-03],\n",
       "        [ 3.32644522e-07, -7.37871731e-37, -2.56962409e-05,\n",
       "         -6.77603227e-35, -8.45396867e-30, -6.53361831e-39,\n",
       "          1.04428222e-07, -5.77655038e-32,  4.55757307e-07,\n",
       "         -1.65234364e-34, -1.61655887e-05,  1.46256662e-06,\n",
       "         -1.45054093e-26,  2.58680617e-05, -4.91418145e-10,\n",
       "         -1.72750425e-08],\n",
       "        [ 1.72015265e-06, -3.52643127e-01, -1.84682493e-06,\n",
       "          9.76603908e-02,  4.26782408e-01,  1.04090290e-06,\n",
       "          5.90095254e-02, -1.31127448e-01, -2.37260412e-01,\n",
       "         -1.08669885e-05, -1.00010740e-05, -7.42598153e-02,\n",
       "          2.76201098e-01, -8.34652010e-38, -4.60246108e-38,\n",
       "          5.84471831e-01],\n",
       "        [-4.94708685e-08, -5.15234283e-01,  2.20757848e-36,\n",
       "          2.92226362e-01,  2.31203501e-01, -1.45441720e-05,\n",
       "          1.44313658e-01, -6.66439970e-02,  4.20380890e-01,\n",
       "          1.00021018e-34,  1.83046723e-08,  3.45903929e-01,\n",
       "          8.15249205e-03, -1.83092259e-08, -2.49379435e-06,\n",
       "         -7.38973218e-02],\n",
       "        [-5.06633273e-06, -1.64448090e-07, -5.83589596e-14,\n",
       "          3.35698737e-23, -2.65866633e-37, -1.82272313e-05,\n",
       "         -1.35877489e-06,  8.36677951e-06, -1.65256830e-37,\n",
       "         -5.84597099e-12,  4.16575189e-18, -2.33244114e-18,\n",
       "         -2.11792218e-11,  1.82316612e-37,  9.35246751e-06,\n",
       "          3.94756821e-09],\n",
       "        [-8.84696358e-16,  9.26432901e-03, -5.98668324e-36,\n",
       "          1.95867831e-01, -1.41487692e-01, -1.45831967e-05,\n",
       "         -4.76710387e-01,  1.31782147e-01,  2.24866798e-01,\n",
       "          4.86851621e-37, -3.25182163e-27,  5.38134430e-01,\n",
       "          3.53194432e-02,  4.79102429e-35, -6.73932521e-38,\n",
       "          2.23299462e-01],\n",
       "        [ 4.12008133e-29, -1.64138895e-01, -1.81592489e-06,\n",
       "         -1.72082895e-01,  2.25148326e-01, -1.68273211e-12,\n",
       "          1.42940905e-01, -2.72125390e-01, -1.04856076e-01,\n",
       "          2.17935122e-11, -2.87749325e-11,  2.05154918e-01,\n",
       "          1.48359393e-02,  1.14585911e-06,  5.62370053e-20,\n",
       "         -4.17829606e-01],\n",
       "        [ 1.44379505e-20,  2.57688150e-01,  4.36473101e-07,\n",
       "          3.32688212e-01, -3.29935853e-01, -5.46962499e-15,\n",
       "          4.62013241e-02,  4.65442671e-01,  4.66273695e-01,\n",
       "          1.51674012e-37,  6.89134365e-07,  1.83829189e-01,\n",
       "         -9.28123236e-02,  1.95323213e-38,  2.98703584e-35,\n",
       "         -3.39509927e-02],\n",
       "        [ 1.23287817e-06,  8.22095669e-07, -1.82320075e-35,\n",
       "          7.08536853e-06, -4.04491074e-10, -4.18515282e-08,\n",
       "         -2.05722244e-08,  8.15518947e-40, -1.30371538e-05,\n",
       "          3.61905748e-06,  4.60400331e-10, -1.66528548e-05,\n",
       "         -3.07050675e-10,  3.72878322e-30, -2.88399833e-08,\n",
       "          4.44978220e-11],\n",
       "        [ 1.26078927e-30,  8.10488540e-01,  1.58281902e-36,\n",
       "          1.55275240e-01,  6.03163604e-01,  3.76683291e-07,\n",
       "         -2.33205701e-01,  2.69376888e-01,  5.23410378e-01,\n",
       "         -1.33883977e-23, -2.93025246e-41,  3.15967443e-01,\n",
       "         -2.62272529e-01,  1.46402228e-37, -3.88610360e-37,\n",
       "          1.73269282e-01],\n",
       "        [-1.24455890e-27,  1.68744113e-07,  2.66605323e-37,\n",
       "         -1.11215760e-10,  1.16715687e-12, -9.30036552e-35,\n",
       "         -3.31733662e-06,  7.29461253e-37, -1.68093682e-29,\n",
       "          8.16074640e-07, -2.79654643e-05,  1.22032447e-05,\n",
       "          2.15610515e-34,  1.08646456e-05,  2.77351256e-35,\n",
       "          5.65773193e-10],\n",
       "        [ 5.03395931e-37,  1.83126878e-01, -9.30180401e-24,\n",
       "          1.21504143e-01,  2.05175215e-01,  1.16063903e-08,\n",
       "          2.39324899e-01,  1.54276229e-01,  3.11494368e-01,\n",
       "          5.61661781e-28,  1.00549604e-28, -5.83799012e-02,\n",
       "         -1.94562366e-02, -2.57883296e-37, -6.96526256e-36,\n",
       "         -3.69091139e-01],\n",
       "        [ 6.01607280e-34, -1.50960061e-01,  1.08725813e-05,\n",
       "          3.28871630e-01, -4.47839409e-01,  6.31826627e-37,\n",
       "          5.40932515e-02,  6.07828658e-02,  1.26017463e-01,\n",
       "          3.42338481e-07,  1.89061588e-30, -3.96264504e-01,\n",
       "         -2.23953175e-01,  8.51975920e-09,  3.66474614e-38,\n",
       "          6.38344183e-01],\n",
       "        [ 3.68905892e-37,  3.40090000e-20, -2.84152311e-38,\n",
       "          2.43035370e-14,  4.89093637e-37,  7.34627013e-38,\n",
       "          1.33017907e-30, -1.65102419e-06, -5.76888570e-34,\n",
       "         -9.00450099e-07,  1.53662166e-05, -1.53308181e-10,\n",
       "          6.26336843e-11,  4.62765960e-08, -2.01824528e-26,\n",
       "          2.46255430e-37]]),\n",
       " array([[-1.91604769e-34],\n",
       "        [ 5.96750381e-01],\n",
       "        [-2.14299852e-08],\n",
       "        [-4.08511144e-01],\n",
       "        [ 1.71589768e-01],\n",
       "        [ 6.52732208e-38],\n",
       "        [-4.07227701e-01],\n",
       "        [ 2.36211647e-01],\n",
       "        [ 2.01545417e-01],\n",
       "        [ 3.24117792e-23],\n",
       "        [-1.83841295e-05],\n",
       "        [ 4.63511865e-01],\n",
       "        [-3.84973866e-01],\n",
       "        [-4.63823239e-38],\n",
       "        [ 1.52816626e-07],\n",
       "        [-7.76647683e-01]])]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.intercepts_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([ 0.52066008, -0.09417066,  0.21362481, -0.2130242 ,  0.24053761,\n",
       "         0.33548159, -0.55623279,  0.42209894,  0.57994716,  0.34176399,\n",
       "        -0.25352144,  0.4329726 , -0.45815514,  0.01946621,  0.67321439,\n",
       "        -0.23831385]),\n",
       " array([-0.29360735,  0.13340547, -0.13408494, -0.30687235,  0.21265764,\n",
       "        -0.16257909,  0.26636635,  0.46399497, -0.08385267, -0.33697802,\n",
       "        -0.26590207,  0.10512076,  0.19207277, -0.25271103, -0.21820934,\n",
       "         0.26684657]),\n",
       " array([-0.10957755])]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model.coefs_[0].shape[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import tinbasic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "out = tinbasic.mlpreg_tibasic(model, \"formula\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max(0,{L1(1)*(0-.325)+L1(2)*(0-.046)+0.521,L1(1)*.356+L1(2)*.104-0.094,L1(1)*(0-.535)+L1(2)*.322+0.214,L1(1)*(0-.224)+L1(2)*.316+0.241,L1(1)*(0-.421)+L1(2)*.557+0.335,L1(1)*(0-.261)+L1(2)*.245+0.422,L1(1)*(0-.03)+L1(2)*.523+0.58,L1(1)*.055+L1(2)*.339+0.342,L1(1)*.409+L1(2)*(0-.842)+0.433,L1(1)*.56+L1(2)*.388+0.019,L1(1)*(0-.673)+L1(2)*(0-.673)+0.673->L1\n",
      "max(0,{L1(1)*(0-.163)+L1(2)*.155+L1(3)*(0-.19)+L1(4)*(0-.353)+L1(5)*(0-.515)+L1(6)*.009+L1(7)*(0-.164)+L1(8)*.258+L1(9)*.81+L1(10)*.183+L1(11)*(0-.151)+0.133,L1(1)*.052+L1(2)*.165+L1(3)*(0-.384)+L1(4)*.098+L1(5)*.292+L1(6)*.196+L1(7)*(0-.172)+L1(8)*.333+L1(9)*.155+L1(10)*.122+L1(11)*.329-0.307,L1(1)*(0-.07)+L1(2)*.182+L1(3)*.037+L1(4)*.427+L1(5)*.231+L1(6)*(0-.141)+L1(7)*.225+L1(8)*(0-.33)+L1(9)*.603+L1(10)*.205+L1(11)*(0-.448)+0.213,L1(1)*(0-.137)+L1(2)*.465+L1(3)*(0-.335)+L1(4)*.059+L1(5)*.144+L1(6)*(0-.477)+L1(7)*.143+L1(8)*.046+L1(9)*(0-.233)+L1(10)*.239+L1(11)*.054+0.266,L1(1)*(0-.238)+L1(2)*(0-.453)+L1(3)*.633+L1(4)*(0-.131)+L1(5)*(0-.067)+L1(6)*.132+L1(7)*(0-.272)+L1(8)*.465+L1(9)*.269+L1(10)*.154+L1(11)*.061+0.464,L1(1)*.23+L1(2)*(0-.465)+L1(3)*.367+L1(4)*(0-.237)+L1(5)*.42+L1(6)*.225+L1(7)*(0-.105)+L1(8)*.466+L1(9)*.523+L1(10)*.311+L1(11)*.126-0.084,L1(1)*.317+L1(2)*(0-.431)+L1(3)*.641+L1(4)*(0-.074)+L1(5)*.346+L1(6)*.538+L1(7)*.205+L1(8)*.184+L1(9)*.316+L1(10)*(0-.058)+L1(11)*(0-.396)+0.105,L1(1)*(0-.388)+L1(2)*.514+L1(3)*(0-.266)+L1(4)*.276+L1(5)*.008+L1(6)*.035+L1(7)*.015+L1(8)*(0-.093)+L1(9)*(0-.262)+L1(10)*(0-.019)+L1(11)*(0-.224)+0.192,L1(1)*(0-.082)+L1(2)*.057+L1(3)*(0-.005)+L1(4)*.584+L1(5)*(0-.074)+L1(6)*.223+L1(7)*(0-.418)+L1(8)*(0-.034)+L1(9)*.173+L1(10)*(0-.369)+L1(11)*.638+0.267->L1\n",
      "max(0,{L1(1)*.597+L1(2)*(0-.409)+L1(3)*.172+L1(4)*(0-.407)+L1(5)*.236+L1(6)*.202+L1(7)*.464+L1(8)*(0-.385)+L1(9)*(0-.777)-0.11->L1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(tinbasic.nn_formula_write_archived(model.coefs_[0],model.intercepts_[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UnArchive L2\n",
      "{L2(1)*(0-.325)+0.521,L2(1)*.356-0.094,L2(1)*(0-.535)+0.214,0,L2(1)*(0-.224)+0.241,L2(1)*(0-.421)+0.335,0,L2(1)*(0-.261)+0.422,L2(1)*(0-.03)+0.58,L2(1)*.055+0.342,0,L2(1)*.409+0.433,0,L2(1)*.56+0.019,L2(1)*(0-.673)+0.673,0,->L1\n",
      "ClrList L2\n",
      "UnArchive L3\n",
      "L3(1)*(0-.046)+L1(1)->L1(1)\n",
      "L3(1)*.104+L1(2)->L1(2)\n",
      "L3(1)*.322+L1(3)->L1(3)\n",
      "L3(1)*.316+L1(5)->L1(5)\n",
      "L3(1)*.557+L1(6)->L1(6)\n",
      "L3(1)*.245+L1(8)->L1(8)\n",
      "L3(1)*.523+L1(9)->L1(9)\n",
      "L3(1)*.339+L1(10)->L1(10)\n",
      "L3(1)*(0-.842)+L1(12)->L1(12)\n",
      "L3(1)*.388+L1(14)->L1(14)\n",
      "L3(1)*(0-.673)+L1(15)->L1(15)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "out = tinbasic.nn_formula_write_archived(model.coefs_[0],model.intercepts_[0]) + \"\\n\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "out += tinbasic.nn_formula_simplify(model.coefs_[1:],model.intercepts_[1:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UnArchive L2\n",
      "{L2(1)*(0-.325)+0.521,L2(1)*.356-0.094,L2(1)*(0-.535)+0.214,0,L2(1)*(0-.224)+0.241,L2(1)*(0-.421)+0.335,0,L2(1)*(0-.261)+0.422,L2(1)*(0-.03)+0.58,L2(1)*.055+0.342,0,L2(1)*.409+0.433,0,L2(1)*.56+0.019,L2(1)*(0-.673)+0.673,0,->L1\n",
      "ClrList L2\n",
      "UnArchive L3\n",
      "L3(1)*(0-.046)+L1(1)->L1(1)\n",
      "L3(1)*.104+L1(2)->L1(2)\n",
      "L3(1)*.322+L1(3)->L1(3)\n",
      "L3(1)*.316+L1(5)->L1(5)\n",
      "L3(1)*.557+L1(6)->L1(6)\n",
      "L3(1)*.245+L1(8)->L1(8)\n",
      "L3(1)*.523+L1(9)->L1(9)\n",
      "L3(1)*.339+L1(10)->L1(10)\n",
      "L3(1)*(0-.842)+L1(12)->L1(12)\n",
      "L3(1)*.388+L1(14)->L1(14)\n",
      "L3(1)*(0-.673)+L1(15)->L1(15)max(0,{L1(1)*(0-.163)+L1(2)*.155+L1(3)*(0-.19)+L1(5)*(0-.353)+L1(6)*(0-.515)+L1(8)*.009+L1(9)*(0-.164)+L1(10)*.258+L1(12)*.81+L1(14)*.183+L1(15)*(0-.151)+0.133,L1(1)*.052+L1(2)*.165+L1(3)*(0-.384)+L1(5)*.098+L1(6)*.292+L1(8)*.196+L1(9)*(0-.172)+L1(10)*.333+L1(12)*.155+L1(14)*.122+L1(15)*.329-0.307,L1(1)*(0-.07)+L1(2)*.182+L1(3)*.037+L1(5)*.427+L1(6)*.231+L1(8)*(0-.141)+L1(9)*.225+L1(10)*(0-.33)+L1(12)*.603+L1(14)*.205+L1(15)*(0-.448)+0.213,L1(1)*(0-.137)+L1(2)*.465+L1(3)*(0-.335)+L1(5)*.059+L1(6)*.144+L1(8)*(0-.477)+L1(9)*.143+L1(10)*.046+L1(12)*(0-.233)+L1(14)*.239+L1(15)*.054+0.266,L1(1)*(0-.238)+L1(2)*(0-.453)+L1(3)*.633+L1(5)*(0-.131)+L1(6)*(0-.067)+L1(8)*.132+L1(9)*(0-.272)+L1(10)*.465+L1(12)*.269+L1(14)*.154+L1(15)*.061+0.464,L1(1)*.23+L1(2)*(0-.465)+L1(3)*.367+L1(5)*(0-.237)+L1(6)*.42+L1(8)*.225+L1(9)*(0-.105)+L1(10)*.466+L1(12)*.523+L1(14)*.311+L1(15)*.126-0.084,L1(1)*.317+L1(2)*(0-.431)+L1(3)*.641+L1(5)*(0-.074)+L1(6)*.346+L1(8)*.538+L1(9)*.205+L1(10)*.184+L1(12)*.316+L1(14)*(0-.058)+L1(15)*(0-.396)+0.105,L1(1)*(0-.388)+L1(2)*.514+L1(3)*(0-.266)+L1(5)*.276+L1(6)*.008+L1(8)*.035+L1(9)*.015+L1(10)*(0-.093)+L1(12)*(0-.262)+L1(14)*(0-.019)+L1(15)*(0-.224)+0.192,L1(1)*(0-.082)+L1(2)*.057+L1(3)*(0-.005)+L1(5)*.584+L1(6)*(0-.074)+L1(8)*.223+L1(9)*(0-.418)+L1(10)*(0-.034)+L1(12)*.173+L1(14)*(0-.369)+L1(15)*.638+0.267->L1\n",
      "max(0,{L1(1)*.597+L1(2)*(0-.409)+L1(3)*.172+L1(4)*(0-.407)+L1(5)*.236+L1(6)*.202+L1(7)*.464+L1(8)*(0-.385)+L1(9)*(0-.777)-0.11->L1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "interpreter": {
   "hash": "c4a6cc1c2df5ddd62d6925b2a7bdee9abacf912eab37272999970e810b9642fd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}